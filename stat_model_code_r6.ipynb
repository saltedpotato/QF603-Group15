{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "286bb4e1",
      "metadata": {},
      "source": [
        "# üìà Volatility Forecasting Analysis with Automated Report Generation\n",
        "\n",
        "## ‚ú® New Features Added\n",
        "\n",
        "This notebook now includes a **comprehensive automated report generation system** that creates a professional markdown document with all your analysis results, plots, and tables.\n",
        "\n",
        "### What's New:\n",
        "\n",
        "1. **VolatilityReportGenerator Class** - A sophisticated report generator that:\n",
        "   - Creates structured markdown reports with Table of Contents\n",
        "   - Automatically saves all plots as PNG images\n",
        "   - Formats tables in markdown\n",
        "   - Organizes outputs in a professional layout\n",
        "   - Ready for conversion to PDF, HTML, or Word\n",
        "\n",
        "2. **Automated Plot Saving** - Every plot generated in the analysis is:\n",
        "   - Saved as high-resolution PNG (150 DPI)\n",
        "   - Embedded in the markdown report\n",
        "   - Captioned with descriptive titles\n",
        "   - Organized in a dedicated images folder\n",
        "\n",
        "3. **Comprehensive Report Sections**:\n",
        "   - Executive Summary\n",
        "   - Data Description\n",
        "   - Methodology\n",
        "   - Volatility Estimators Analysis (with diagnostics)\n",
        "   - HAR Model Results (all windows)\n",
        "   - HAR-X Model Results (with exogenous variables)\n",
        "   - Ensemble Model Analysis\n",
        "   - Statistical Tests (Diebold-Mariano, Ljung-Box)\n",
        "   - Test Set Evaluation\n",
        "   - Conclusions and Recommendations\n",
        "   - Appendix\n",
        "\n",
        "### How to Use:\n",
        "\n",
        "1. **Run all cells** in this notebook from top to bottom\n",
        "2. **Find your report** in the `report_output/` directory\n",
        "3. **Open the markdown file** to view your comprehensive analysis\n",
        "4. **Convert to PDF/HTML/Word** using Pandoc if needed\n",
        "\n",
        "### Output Structure:\n",
        "```\n",
        "report_output/\n",
        "‚îú‚îÄ‚îÄ volatility_forecast_report_YYYYMMDD_HHMMSS.md\n",
        "‚îî‚îÄ‚îÄ images/\n",
        "    ‚îú‚îÄ‚îÄ acf_square_est_log.png\n",
        "    ‚îú‚îÄ‚îÄ pacf_square_est_log.png\n",
        "    ‚îú‚îÄ‚îÄ har_prediction_w252.png\n",
        "    ‚îú‚îÄ‚îÄ qlike_loss_w504.png\n",
        "    ‚îî‚îÄ‚îÄ ... (60+ plots)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to generate your professional research report? Run all cells below! ‚¨áÔ∏è**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e92f77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Volatility Report Generator\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "class VolatilityReportGenerator:\n",
        "    \"\"\"\n",
        "    A comprehensive report generator for volatility forecasting analysis.\n",
        "    Saves plots and outputs in structured markdown format for presentation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, report_name=\"volatility_forecast_report\"):\n",
        "        self.report_name = report_name\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        \n",
        "        # Create directories\n",
        "        self.base_dir = Path(\"./report_output_v6\")\n",
        "        self.images_dir = self.base_dir / \"images\"\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "        self.images_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Report file\n",
        "        self.report_file = self.base_dir / f\"{report_name}_{self.timestamp}.md\"\n",
        "        \n",
        "        # Initialize report\n",
        "        self._init_report()\n",
        "        \n",
        "    def _init_report(self):\n",
        "        \"\"\"Initialize the markdown report with title and TOC\"\"\"\n",
        "        with open(self.report_file, 'w') as f:\n",
        "            f.write(f\"# Volatility Forecasting Report\\n\\n\")\n",
        "            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "            f.write(f\"**Author:** PhD Research Team\\n\\n\")\n",
        "            f.write(f\"---\\n\\n\")\n",
        "            f.write(f\"## Table of Contents\\n\\n\")\n",
        "            f.write(f\"1. [Executive Summary](#executive-summary)\\n\")\n",
        "            f.write(f\"2. [Data Description](#data-description)\\n\")\n",
        "            f.write(f\"3. [Methodology](#methodology)\\n\")\n",
        "            f.write(f\"4. [Volatility Estimators Analysis](#volatility-estimators-analysis)\\n\")\n",
        "            f.write(f\"5. [HAR Model Results](#har-model-results)\\n\")\n",
        "            f.write(f\"6. [HAR-X Model Results](#har-x-model-results)\\n\")\n",
        "            f.write(f\"7. [Model Comparison](#model-comparison)\\n\")\n",
        "            f.write(f\"8. [Test Set Evaluation](#test-set-evaluation)\\n\")\n",
        "            f.write(f\"9. [Conclusions](#conclusions)\\n\")\n",
        "            f.write(f\"10. [Appendix](#appendix)\\n\\n\")\n",
        "            f.write(f\"---\\n\\n\")\n",
        "    \n",
        "    def add_section(self, title, level=2):\n",
        "        \"\"\"Add a section header\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            f.write(f\"\\n{'#' * level} {title}\\n\\n\")\n",
        "    \n",
        "    def add_text(self, text):\n",
        "        \"\"\"Add text content\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            f.write(f\"{text}\\n\\n\")\n",
        "    \n",
        "    def add_table(self, df, caption=\"\"):\n",
        "        \"\"\"Add a pandas DataFrame as markdown table\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            if caption:\n",
        "                f.write(f\"**{caption}**\\n\\n\")\n",
        "            f.write(df.to_markdown())\n",
        "            f.write(\"\\n\\n\")\n",
        "    \n",
        "    def save_and_add_plot(self, fig, filename, caption=\"\", width=800):\n",
        "        \"\"\"Save matplotlib figure and add to report\"\"\"\n",
        "        # Save figure\n",
        "        img_path = self.images_dir / f\"{filename}.png\"\n",
        "        fig.savefig(img_path, dpi=150, bbox_inches='tight')\n",
        "        \n",
        "        # Add to report\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            if caption:\n",
        "                f.write(f\"**{caption}**\\n\\n\")\n",
        "            f.write(f\"![{caption}](images/{filename}.png)\\n\\n\")\n",
        "        \n",
        "        print(f\"‚úì Saved plot: {filename}.png\")\n",
        "        return str(img_path)\n",
        "    \n",
        "    def add_metrics_summary(self, metrics_dict, title=\"Metrics Summary\"):\n",
        "        \"\"\"Add metrics in a formatted way\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            f.write(f\"**{title}**\\n\\n\")\n",
        "            f.write(\"| Metric | Value |\\n\")\n",
        "            f.write(\"|--------|-------|\\n\")\n",
        "            for key, value in metrics_dict.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    f.write(f\"| {key} | {value:.4f} |\\n\")\n",
        "                else:\n",
        "                    f.write(f\"| {key} | {value} |\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "    \n",
        "    def add_code_output(self, output, title=\"\"):\n",
        "        \"\"\"Add code output in formatted code block\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            if title:\n",
        "                f.write(f\"**{title}**\\n\\n\")\n",
        "            f.write(\"```\\n\")\n",
        "            f.write(str(output))\n",
        "            f.write(\"\\n```\\n\\n\")\n",
        "    \n",
        "    def finalize_report(self):\n",
        "        \"\"\"Finalize the report\"\"\"\n",
        "        with open(self.report_file, 'a') as f:\n",
        "            f.write(f\"\\n---\\n\\n\")\n",
        "            f.write(f\"*Report generated on {datetime.now().strftime('%Y-%m-%d at %H:%M:%S')}*\\n\")\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"‚úì Report generated successfully!\")\n",
        "        print(f\"  Location: {self.report_file}\")\n",
        "        print(f\"  Images:   {self.images_dir}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Initialize the report generator\n",
        "report = VolatilityReportGenerator()\n",
        "print(\"Report generator initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63054ceb",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Quick Reference: Report Generation Commands\n",
        "\n",
        "The `VolatilityReportGenerator` class provides these key methods:\n",
        "\n",
        "### Core Methods:\n",
        "\n",
        "```python\n",
        "# Initialize the report generator\n",
        "report = VolatilityReportGenerator(report_name=\"my_analysis\")\n",
        "\n",
        "# Add section headers\n",
        "report.add_section(\"Section Title\", level=2)  # level 2-4 for subsections\n",
        "\n",
        "# Add text content\n",
        "report.add_text(\"Your analysis description here...\")\n",
        "\n",
        "# Add pandas DataFrame as table\n",
        "report.add_table(dataframe, caption=\"Table description\")\n",
        "\n",
        "# Save and embed a matplotlib figure\n",
        "report.save_and_add_plot(fig, \"filename\", caption=\"Plot description\")\n",
        "\n",
        "# Add metrics summary\n",
        "report.add_metrics_summary({\"Metric1\": value1, \"Metric2\": value2}, \n",
        "                          title=\"Metrics Title\")\n",
        "\n",
        "# Add code output\n",
        "report.add_code_output(output_text, title=\"Output Title\")\n",
        "\n",
        "# Finalize report\n",
        "report.finalize_report()\n",
        "```\n",
        "\n",
        "### File Structure:\n",
        "\n",
        "All outputs are saved to: `./report_output/`\n",
        "- Main report: `volatility_forecast_report_TIMESTAMP.md`\n",
        "- All images: `./report_output/images/*.png`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97837325",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97837325",
        "outputId": "fd1fb124-3230-4cff-d6a4-5e02d1256fc8"
      },
      "outputs": [],
      "source": [
        "# !pip install statsmodels\n",
        "# !pip install lets-plot\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0eef23a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and install required packages for report generation\n",
        "import sys\n",
        "\n",
        "required_packages = {\n",
        "    'tabulate': 'tabulate',  # for markdown table generation\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'pandas': 'pandas',\n",
        "    'numpy': 'numpy'\n",
        "}\n",
        "\n",
        "missing_packages = []\n",
        "\n",
        "for package_name, pip_name in required_packages.items():\n",
        "    try:\n",
        "        __import__(package_name)\n",
        "        print(f\"‚úì {package_name} is installed\")\n",
        "    except ImportError:\n",
        "        print(f\"‚úó {package_name} is NOT installed\")\n",
        "        missing_packages.append(pip_name)\n",
        "\n",
        "if missing_packages:\n",
        "    print(f\"\\n‚ö†Ô∏è  Installing missing packages: {', '.join(missing_packages)}\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
        "    print(\"‚úì All packages installed successfully!\")\n",
        "else:\n",
        "    print(\"\\n‚úì All required packages are installed!\")\n",
        "    print(\"\\nüìä Ready to generate comprehensive volatility forecasting report!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162d20e9",
      "metadata": {
        "id": "162d20e9"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, acf as sm_acf, pacf as sm_pacf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "from scipy.stats import norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf48a7c6",
      "metadata": {
        "id": "bf48a7c6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "#from pandas_datareader import data as pdr\n",
        "\n",
        "import datetime as dt\n",
        "import yfinance as yf\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import datetime as dt\n",
        "import time\n",
        "import re\n",
        "\n",
        "from lets_plot import *\n",
        "LetsPlot.setup_html()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762d808b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "762d808b",
        "outputId": "a6baf1b4-79ad-447b-b425-9318fcc4f147"
      },
      "outputs": [],
      "source": [
        "data_folder = 'data'\n",
        "# Read into DataFrame\n",
        "IV_y_values = pd.read_csv(f'{data_folder}/MOVE_index.csv')\n",
        "Fed_funds = pd.read_csv(f'{data_folder}/FedFunds.csv')\n",
        "UST_10Y = pd.read_csv(f'{data_folder}/UST10Y.csv')\n",
        "HYOAS = pd.read_csv(f'{data_folder}/HYOAS.csv')\n",
        "NFCI = pd.read_csv(f'{data_folder}/NFCI.csv')\n",
        "Termspread = pd.read_csv(f'{data_folder}/TermSpread_10Y_2Y.csv')\n",
        "vix = pd.read_csv(f'{data_folder}/VIX.csv')\n",
        "Breakeven_10Y = pd.read_csv(f'{data_folder}/Breakeven10Y.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a717f95",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ThjJWSu1WCU_",
      "metadata": {
        "id": "ThjJWSu1WCU_"
      },
      "source": [
        "## RV FORECASTING CODE STRUCTURE\n",
        "\n",
        "- Outer Loop: Rolling Window Sizes: Iterate over different rolling window sizes to test how each window size affects model performance.\n",
        "\n",
        "- Second Loop: Feature Construction for All Estimators. For each window size, compute features using your HAR_Model.features() method. Each estimator (e.g., RV, BV, MedRV, RR) will have its own feature set derived from its volatility series.\n",
        "\n",
        "- Third Loop: Fit & Predict for Each Estimator. For each estimator: Run fit_predict() using the features and target series. Collect yhat (forecasted volatility) and residuals. Store predictions and residuals for metric evaluation.\n",
        "\n",
        "- End of Loop.\n",
        "\n",
        "Metric Computation. For each estimator: Compute QLIKE, MPSE, and optionally other metrics. Plot: QLIKE and MPSE over time. QLIKE mean and variance over time (to assess stability). These plots help visualize performance across time and windows. Residual to be evaluted. Do the same for the ensemble model with weightage to be computed based on metric performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bd00b5",
      "metadata": {
        "id": "08bd00b5"
      },
      "outputs": [],
      "source": [
        "class volatility_estimator:\n",
        "    def __init__(self, add_log):\n",
        "        self.add_log = add_log\n",
        "\n",
        "    def _check(self, df):\n",
        "        required = ['High', 'Low', 'Open', 'Close']\n",
        "        if not set(required).issubset (df.columns):\n",
        "            raise ValueError(f\"Dataframe needs columns {required}.\")\n",
        "        if (df[required]<=0).any().any():\n",
        "            raise ValueError(f\"Dataframe contains nonpositive values\")\n",
        "        return df\n",
        "\n",
        "    def compute_square_return(self,df):\n",
        "        df = self._check(df)\n",
        "        log_return =  np.log(df['Close'] / df['Close'].shift(1))\n",
        "        return 252*(log_return ** 2)\n",
        "\n",
        "    def compute_parkinson_estimator(self,df):\n",
        "        df = self._check(df)\n",
        "        log_par_var = (np.log(df['High'] / df['Low']))**2\n",
        "        return 252*((1/(4*np.log(2))) * log_par_var)\n",
        "\n",
        "    def compute_gk_estimator(self,df):\n",
        "        df = self._check(df)\n",
        "        gk_var_1 = (1/2)*(np.log(df['High']/df['Low']))**2\n",
        "        gk_var_2 = (2*np.log(2)-1)*(np.log(df['Close']/df['Open']))**2\n",
        "        return 252*(gk_var_1 - gk_var_2)\n",
        "\n",
        "    def compute_rs_estimator(self, df):\n",
        "        df = self._check(df)\n",
        "        rs_var_1 = (np.log(df['High']/df['Open']))*(np.log(df['High']/df['Close']))\n",
        "        rs_var_2 = (np.log(df['Low']/df['Open']))*(np.log(df['Low']/df['Close']))\n",
        "        return 252*(rs_var_1 + rs_var_2)\n",
        "\n",
        "    def compute_all(self,df, lag_for_predictors:bool=False):\n",
        "          df = self._check(df).copy()\n",
        "          eps = 1e-12\n",
        "\n",
        "          out = pd.DataFrame(index = df.index)\n",
        "          out['square_est'] = self.compute_square_return(df)\n",
        "          out['parkinson_est']=self.compute_parkinson_estimator(df)\n",
        "          out['gk_est'] = self.compute_gk_estimator(df)\n",
        "          out['rs_est'] = self.compute_rs_estimator(df)\n",
        "\n",
        "          if self.add_log:\n",
        "              for col in ['square_est', 'parkinson_est', 'gk_est', 'rs_est']:\n",
        "                  x = out[col].astype(float).replace([np.inf, -np.inf], np.nan)\n",
        "                  out[col + '_log'] = np.log(x.clip(lower=eps))\n",
        "          if lag_for_predictors:\n",
        "            out = out.shift(1)\n",
        "\n",
        "          return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47194e9e",
      "metadata": {
        "id": "47194e9e"
      },
      "outputs": [],
      "source": [
        "# premodel diagnotics on the data assumptions.\n",
        "import warnings\n",
        "\n",
        "class Vol_Est_Check:\n",
        "\n",
        "    def __init__(self,\n",
        "                 alpha,\n",
        "                 lb_lags,\n",
        "                 kpss_reg,\n",
        "                 kpss_nlags,\n",
        "                 acf_pacf_nlags):\n",
        "        # alpha: significant level\n",
        "        # lb_lags: lags to report to Ljung box\n",
        "        # kpss: reg[c - level, ct - trend], nlags: auto or int\n",
        "        #ADF passed stationary when p<0.05 (reject H_0 of unit root)\n",
        "        self.alpha = alpha\n",
        "        self.lb_lags = tuple(lb_lags)\n",
        "        self.kpss_reg = kpss_reg\n",
        "        self.kpss_nlags = kpss_nlags\n",
        "        self.acf_pacf_nlags = acf_pacf_nlags\n",
        "\n",
        "    def ADF(self, df, name):\n",
        "        df = df.dropna()\n",
        "        series_name = name or getattr (df, 'name', 'series')\n",
        "\n",
        "        stat, p, lags, nobs, crit, icbest = adfuller(df, autolag = 'AIC')\n",
        "        stationary_flag = p <= self.alpha\n",
        "        return {\"adf_stat\": stat,\n",
        "                \"adf_p\": p,\n",
        "                \"adf_lags\": lags,\n",
        "                \"adf_nobs\": nobs,\n",
        "                \"adf_crit\": crit,\n",
        "                'adf_icbest': icbest,\n",
        "                'adf_stationary_flag': stationary_flag,\n",
        "                'adf_decision': (f'{series_name}: Reject H0 -> stationary'\n",
        "                    if stationary_flag\n",
        "                    else 'Fail to reject H0 -> non-stationary'\n",
        "                )}\n",
        "\n",
        "\n",
        "    def KPSS(self,df, name, nlags):\n",
        "        df = df.dropna()\n",
        "        series_name = name or getattr (df, 'name', 'series')\n",
        "\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            stat, p, lags, crit = kpss(df,\n",
        "                                       regression=self.kpss_reg,\n",
        "                                       nlags=nlags)\n",
        "\n",
        "        stationary_flag = (p > self.alpha)\n",
        "        return {\n",
        "            \"kpss_stat\": stat, # stat < crit -> series is stationary\n",
        "            \"kpss_p\": p, # p >0.05  -> series is stationary\n",
        "            \"kpss_lags\": lags,\n",
        "            \"kpss_crit\": crit,\n",
        "            'kpss_reg': self.kpss_reg,\n",
        "            'kpss_stationary_flag': stationary_flag,\n",
        "            'kpss_decision': (f'{series_name}: Fail to reject H0 ->stationary'\n",
        "            if stationary_flag\n",
        "            else f\"{series_name}: Reject H0 -> Non-stationary\")\n",
        "            }\n",
        "\n",
        "\n",
        "    def ljung_box(self, df): # reject H0 -> serial correlation\n",
        "        df = df.dropna()\n",
        "        lb = acorr_ljungbox(df, lags=list(self.lb_lags), return_df=True)\n",
        "        out={}\n",
        "        for L in self.lb_lags:\n",
        "            out[f'lb_stat_{L}'] = float(lb.loc[L, \"lb_stat\"]) # stat for each lag h\n",
        "            out[f\"lb_p_{L}\"]   = float(lb.loc[L, \"lb_pvalue\"])  # p-value\n",
        "\n",
        "        out['white_noise_flag'] = all(out[f'lb_p_{L}'] > self.alpha for L in self.lb_lags)\n",
        "        out[\"lb_lags_used\"] = self.lb_lags\n",
        "        out['n_obs'] = len(df)\n",
        "        out['name'] = getattr(df, 'name', 'series')\n",
        "        return out\n",
        "\n",
        "    def compute_acf(self, df, nlags, alpha):\n",
        "        df=df.dropna()\n",
        "        nlags = nlags or self.acf_pacf_nlags\n",
        "        vals, conf = sm_acf(df,\n",
        "                            nlags = nlags,\n",
        "                            alpha = alpha,\n",
        "                            fft = True,\n",
        "                            adjusted = False)\n",
        "        return {'acf_vals': vals,\n",
        "                'acf_confint': conf} #shape(nlags+1,2)\n",
        "\n",
        "\n",
        "    def compute_pacf(self, df, nlags, alpha, method: str = 'ywmle'):\n",
        "        df=df.dropna()\n",
        "        nlags = nlags or self.acf_pacf_nlags\n",
        "        vals, conf = sm_pacf(df,\n",
        "                             nlags=nlags,\n",
        "                             alpha=alpha,\n",
        "                             method=method)\n",
        "        return {\n",
        "            'pacf_vals': vals,\n",
        "            'pacf_confint': conf,\n",
        "            'pacf_method': method\n",
        "            }\n",
        "\n",
        "    def plot_acf(self, df, nlags, title:str = None):\n",
        "        df = df.dropna()\n",
        "        nlags = nlags or self.acf_pacf_nlags\n",
        "        plot_acf(df, lags=nlags)\n",
        "        plt.title(title or f\"ACF ({getattr(df,'name','series')})\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_pacf(self, df, nlags, title: str = None, method: str = 'ywmle'):\n",
        "        df = df.dropna()\n",
        "        nlags = nlags or self.acf_pacf_nlags\n",
        "        plot_pacf(df, lags=nlags)\n",
        "        plt.title(title or f\"PACF ({getattr(df,'name','series')})\")\n",
        "        plt.show()\n",
        "\n",
        "    def summarize_series(self, df, name = None): #for 1 estimator\n",
        "        series_name = name or getattr (df, 'name', 'series')\n",
        "        results = {}\n",
        "        results.update(self.ADF(df, name = name))\n",
        "        results.update(self.KPSS(df, name = name, nlags=self.kpss_nlags))\n",
        "\n",
        "        lb = self.ljung_box(df)\n",
        "        results.update({f\"lb_{k}\": v for k, v in lb.items()})\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832fcdb4",
      "metadata": {
        "id": "832fcdb4"
      },
      "outputs": [],
      "source": [
        "#HAR model function\n",
        "from typing import Iterable, Optional, Dict, Tuple\n",
        "\n",
        "class HAR_Model:\n",
        "  def __init__(self, y_log_col, exo_col, lags =[1,5,22]):\n",
        "    self.y_log_col = y_log_col\n",
        "    self.exo_col = exo_col\n",
        "    self.lags = lags #daily, weekly, monthly\n",
        "\n",
        "  def features(self, df):\n",
        "    y_pred = df[self.y_log_col]\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['rv_d'] = y_pred\n",
        "    out['rv_w'] = y_pred.rolling(self.lags[1], min_periods = self.lags[1]).mean()\n",
        "    out['rv_m'] = y_pred.rolling(self.lags[2], min_periods = self.lags[2]).mean()\n",
        "\n",
        "    if self.exo_col:\n",
        "      for col in self.exo_col:\n",
        "        out[f'x_{col}'] =df[col]\n",
        "\n",
        "    return out.dropna()\n",
        "\n",
        "  def fit_predict(self,\n",
        "                  x_train,\n",
        "                  y_train,\n",
        "                  window):\n",
        "\n",
        "    resid_full = pd.Series(index=y_train.index, data=np.nan)\n",
        "    yhat_full = pd.Series(index=y_train.index, data=np.nan)\n",
        "    residual_raw = pd.Series(index=y_train.index, data=np.nan)\n",
        "    for t in range(window, len(y_train)):\n",
        "      y_slice = y_train.iloc[t-window:t]\n",
        "      x_slice = x_train.iloc[t-window:t]\n",
        "\n",
        "      common_idx = x_slice.index.intersection(y_slice.index)\n",
        "      y_slice = y_train.loc[common_idx]\n",
        "      x_slice = x_train.loc[common_idx]\n",
        "\n",
        "      model = sm.OLS(y_slice, sm.add_constant(x_slice)).fit()\n",
        "\n",
        "      x_next = pd.DataFrame([x_train.iloc[t]])\n",
        "      x_next = sm.add_constant(x_next, has_constant='add')\n",
        "\n",
        "      yhat_full.iloc[t] = model.predict(x_next).iloc[0]\n",
        "      resid_full.iloc[t] = model.resid.var(ddof=x_slice.shape[1]) #ddof degree of freedom correction for unbiased variance\n",
        "      residual_raw.iloc[t] = yhat_full.iloc[t] - y_train.iloc[t]\n",
        "\n",
        "    return yhat_full, resid_full, residual_raw\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VErL5f1qV1bw",
      "metadata": {
        "id": "VErL5f1qV1bw"
      },
      "outputs": [],
      "source": [
        "# to compute for the ensemble model\n",
        "\n",
        "class EnsembleModel:\n",
        "  def __init__(self, estimators):\n",
        "    self.estimators = estimators\n",
        "\n",
        "  def compute_weightage(self, qlike, eps=1e-12): # weightage computed by using inverse qlike\n",
        "    inverse = {k: 1.0/max(v,eps) for k,v in qlike.items()}\n",
        "    total = sum(inverse.values()) if inverse else 0.0\n",
        "    weight = {k: v/total for k,v in inverse.items()}\n",
        "    return weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ypeiwUJ9rrxv",
      "metadata": {
        "id": "ypeiwUJ9rrxv"
      },
      "outputs": [],
      "source": [
        "# metric computation function\n",
        "\n",
        "class Metric_Evaluation:\n",
        "  def __init__(self, ytrue, y_pred, alpha):\n",
        "    self.y_pred = y_pred\n",
        "    self.ytrue = ytrue\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def mspe(ytrue, ypred):\n",
        "    return ((ytrue - ypred) / ytrue) ** 2\n",
        "\n",
        "  def qlike(ytrue, ypred):\n",
        "    return np.log(ypred) + ytrue / ypred\n",
        "\n",
        "  def rmse(ytrue, ypred, window = 22):\n",
        "    errors = (ytrue - ypred)**2\n",
        "    rolling_mean = pd.Series(errors).rolling(window = window).mean()\n",
        "    return np.sqrt(rolling_mean)\n",
        "\n",
        "  def diebold_mariano_test(y_true, pred1, pred2, h=1, loss_type='MSE'):\n",
        "    # \"\"\"\n",
        "    # Diebold-Mariano test for equal predictive accuracY\n",
        "    # Parameters:\n",
        "    # y_true: actual values\n",
        "    # pred1: predictions from model 1\n",
        "    # pred2: predictions from model 2\n",
        "    # h: forecast horizon\n",
        "    # loss_type: 'MSE' or 'QLIKE'\n",
        "    # \"\"\"\n",
        "    if loss_type == 'MSE':\n",
        "        loss1 = (y_true - pred1) ** 2\n",
        "        loss2 = (y_true - pred2) ** 2\n",
        "    elif loss_type == 'QLIKE':\n",
        "        loss1 = np.log(pred1) + y_true / pred1\n",
        "        loss2 = np.log(pred2) + y_true / pred2\n",
        "    else:\n",
        "        raise ValueError(\"loss_type must be 'MSE' or 'QLIKE'\")\n",
        "\n",
        "    d = loss1 - loss2\n",
        "    d = np.asarray(d).flatten()\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "    d_centered = d - d_mean\n",
        "\n",
        "    # HAC variance estimator\n",
        "    L = np.floor(1.5*n**(1/3))\n",
        "    gamma_0 = np.mean(d_centered**2)\n",
        "    for lag in range(1, L+1):\n",
        "      gamma_lag = np.mean(d_centered[:-lag] * d_centered[lag:])\n",
        "      w = 1 - lag / (L + 1)\n",
        "      gamma_0 += 2 * w * gamma_lag\n",
        "\n",
        "    dm_stat = d_mean / np.sqrt(gamma_0 / n)\n",
        "    p_value = 2 * (1 - norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    return dm_stat, p_value\n",
        "\n",
        "\n",
        "  def DM_test(loss1, loss2, alpha = 0.05, model1_name = 'Model 1', model2_name = 'Model_2'): #confirm using qlike\n",
        "    d = loss1 - loss2\n",
        "    d = np.asarray(d).flatten()\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "    d_centered = d - d_mean\n",
        "\n",
        "    # HAC variance estimator\n",
        "    L = int(np.floor(1.5*n**(1/3)))\n",
        "    gamma_0 = np.mean(d_centered**2)\n",
        "    for lag in range(1, L+1):\n",
        "      gamma_lag = np.mean(d_centered[:-lag] * d_centered[lag:])\n",
        "      w = 1 - lag / (L + 1)\n",
        "      gamma_0 += 2 * w * gamma_lag\n",
        "\n",
        "    dm_stat = d_mean / np.sqrt(gamma_0 / n)\n",
        "    p_value = 2 * (1 - norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    #decision logic\n",
        "    if p_value < alpha:\n",
        "       winner = model1_name if dm_stat < 0 else model2_name\n",
        "       significant = True\n",
        "    else:\n",
        "       winner = 'None (No significant difference)'\n",
        "       significant = False\n",
        "    decision ={\n",
        "       'Better model': winner,\n",
        "       'Significant': significant,\n",
        "       'Alpha': alpha,\n",
        "       'Observations': n\n",
        "    }\n",
        "    return dm_stat, p_value, decision\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a4a53b",
      "metadata": {
        "id": "66a4a53b"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nOWaemRClgqj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "nOWaemRClgqj",
        "outputId": "3b66a2bc-2ecf-4f64-91e0-1063523c980a"
      },
      "outputs": [],
      "source": [
        "starting = \"2003-01-01\"\n",
        "ending = \"2025-09-30\"\n",
        "\n",
        "tlt =\\\n",
        "( # one ticker\n",
        "    yf\n",
        "    .download(\"TLT\", # ticker\n",
        "              start = starting, # starting date\n",
        "              end = ending,\n",
        "             auto_adjust = False)\n",
        "    .droplevel(\"Ticker\",\n",
        "                axis = 1)\n",
        "    # [[\"Close\", \"Volume\"]]\n",
        ")\n",
        "\n",
        "tlt_data = tlt.loc[:'2024-12-30']\n",
        "tlt_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AsCtrUomlgmb",
      "metadata": {
        "id": "AsCtrUomlgmb"
      },
      "outputs": [],
      "source": [
        "# tlt.to_csv(\"C:/Users/lawor/OneDrive/Desktop/2025sem3/QF603/project/tlt_data.csv\", index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3sE9kLqdlgcM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "3sE9kLqdlgcM",
        "outputId": "16433342-207d-43a6-cec2-057f67eacd24"
      },
      "outputs": [],
      "source": [
        "# y_true is the next day realized variance that is not known at time t\n",
        "eps = 1e-12\n",
        "\n",
        "y_true =\\\n",
        "(\n",
        "    252\n",
        "    *\n",
        "    (np.log(tlt_data[\"Close\"]\n",
        "           .shift(-1)\n",
        "            /\n",
        "           tlt_data[\"Close\"]))**2\n",
        "\n",
        ")\n",
        "y_true_log = np.log(y_true.clip(lower=eps))\n",
        "y_true_log =\\\n",
        "(\n",
        "    y_true_log\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .dropna()\n",
        "    .iloc[1:]\n",
        ")\n",
        "y_true_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zfPZyGMPYOGv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "zfPZyGMPYOGv",
        "outputId": "aa8a04b0-038f-494e-857b-f2e471666f46"
      },
      "outputs": [],
      "source": [
        "y_true_log.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f643ea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4f643ea4",
        "outputId": "95df8640-b39f-42e5-d8e0-6413a0524118"
      },
      "outputs": [],
      "source": [
        "# to compute the estimators\n",
        "vol_calc = volatility_estimator(add_log=True)\n",
        "vol_results = vol_calc.compute_all(tlt_data, lag_for_predictors=True)\n",
        "vol_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fwwqQQA0fqag",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "fwwqQQA0fqag",
        "outputId": "c0df386d-dc67-4f67-a1f7-95ec1320cebf"
      },
      "outputs": [],
      "source": [
        "vol_results.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rWjDPx7uaUGB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "rWjDPx7uaUGB",
        "outputId": "293f7564-1e22-427f-e645-426c08bf164e"
      },
      "outputs": [],
      "source": [
        "vol_results_adj = vol_results.dropna()\n",
        "vol_results_adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c121505e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "c121505e",
        "outputId": "04c7c097-efeb-4942-dbc9-0a9c31279471"
      },
      "outputs": [],
      "source": [
        "vol_estimator_check = vol_results[['square_est_log',\n",
        "                                  'parkinson_est_log',\n",
        "                                    'gk_est_log',\n",
        "                                    'rs_est_log']]\n",
        "\n",
        "y_predictors = vol_estimator_check.dropna()\n",
        "y_predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CJii7V1NYZQf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "CJii7V1NYZQf",
        "outputId": "34d69dcd-c981-4a67-feaa-7bae523f4875"
      },
      "outputs": [],
      "source": [
        "y_predictors.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73fec4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73fec4e",
        "outputId": "27fcb866-4571-49c9-b2a4-959a1a3b4874"
      },
      "outputs": [],
      "source": [
        "vol_check = Vol_Est_Check(\n",
        "    alpha=0.05,\n",
        "    lb_lags=(10, 20),\n",
        "    kpss_reg='c',\n",
        "    kpss_nlags='auto',\n",
        "    acf_pacf_nlags=40\n",
        ")\n",
        "\n",
        "for col in vol_estimator_check.columns:\n",
        "    print(f\"=== Diagnostics for {col} ===\")\n",
        "    result = vol_check.summarize_series(vol_estimator_check[col], name=col)\n",
        "    print(result, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c076d8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Executive Summary and Introduction to Report\n",
        "report.add_section(\"Executive Summary\", level=2)\n",
        "report.add_text(\"\"\"\n",
        "This report presents a comprehensive analysis of volatility forecasting using Heterogeneous Autoregressive (HAR) \n",
        "and HAR with exogenous variables (HAR-X) models applied to Treasury Bond ETF (TLT) data. \n",
        "\n",
        "**Key Objectives:**\n",
        "- Evaluate multiple volatility estimators (Squared Return, Parkinson, Garman-Klass, Rogers-Satchell)\n",
        "- Compare HAR and HAR-X model performance across different rolling window sizes\n",
        "- Implement ensemble forecasting using inverse QLIKE weighting\n",
        "- Validate models using out-of-sample testing and statistical comparisons\n",
        "\n",
        "**Main Findings:**\n",
        "- Ensemble models outperform individual estimators across all metrics\n",
        "- HAR-X with window=756 provides most stable and consistent predictions\n",
        "- Exogenous variables offer marginal but meaningful improvement in forecast calibration\n",
        "- No statistically significant difference between windows 504 and 756 (DM test)\n",
        "- Both models demonstrate strong forecasting ability with well-behaved residuals\n",
        "\"\"\")\n",
        "\n",
        "report.add_section(\"Data Description\", level=2)\n",
        "report.add_text(f\"\"\"\n",
        "**Dataset:** iShares 20+ Year Treasury Bond ETF (TLT)  \n",
        "**Period:** 2003-01-01 to 2024-12-30  \n",
        "**Frequency:** Daily  \n",
        "**Total Observations:** {len(tlt_data)}  \n",
        "\n",
        "**Price Data Components:**\n",
        "- Open, High, Low, Close prices\n",
        "- Trading volume\n",
        "- Adjusted close prices\n",
        "\n",
        "**Target Variable:**\n",
        "- Realized Volatility (RV): Annualized variance computed from log returns\n",
        "- Log-transformed for modeling to ensure stationarity\n",
        "\n",
        "**Train/Test Split:**\n",
        "- Training Set: 70% of data ({int(0.7 * len(y_predictors))} observations)\n",
        "- Test Set: 30% of data ({len(y_predictors) - int(0.7 * len(y_predictors))} observations)\n",
        "\"\"\")\n",
        "\n",
        "report.add_section(\"Methodology\", level=2)\n",
        "report.add_text(\"\"\"\n",
        "### Model Framework\n",
        "\n",
        "**1. Volatility Estimation**\n",
        "\n",
        "Four volatility estimators are computed from OHLC data:\n",
        "- **Squared Return (RV)**: œÉ¬≤‚Çú = 252 √ó (log(C‚Çú/C‚Çú‚Çã‚ÇÅ))¬≤\n",
        "- **Parkinson**: œÉ¬≤‚Çú = 252 √ó (1/(4ln2)) √ó (log(H‚Çú/L‚Çú))¬≤\n",
        "- **Garman-Klass**: œÉ¬≤‚Çú = 252 √ó [0.5(log(H‚Çú/L‚Çú))¬≤ - (2ln2-1)(log(C‚Çú/O‚Çú))¬≤]\n",
        "- **Rogers-Satchell**: œÉ¬≤‚Çú = 252 √ó [log(H‚Çú/O‚Çú)log(H‚Çú/C‚Çú) + log(L‚Çú/O‚Çú)log(L‚Çú/C‚Çú)]\n",
        "\n",
        "All estimators are log-transformed for modeling.\n",
        "\n",
        "**2. HAR Model**\n",
        "\n",
        "The HAR model captures heterogeneous volatility components:\n",
        "\n",
        "log(RV‚Çú) = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑RV‚Çú‚Çã‚ÇÅ + Œ≤‚ÇÇ¬∑RV‚Çú‚Çã‚ÇÖ:‚Çú‚Çã‚ÇÅ + Œ≤‚ÇÉ¬∑RV‚Çú‚Çã‚ÇÇ‚ÇÇ:‚Çú‚Çã‚ÇÅ + Œµ‚Çú\n",
        "\n",
        "Where:\n",
        "- RV‚Çú‚Çã‚ÇÅ: Daily component (lag 1)\n",
        "- RV‚Çú‚Çã‚ÇÖ:‚Çú‚Çã‚ÇÅ: Weekly component (5-day average)\n",
        "- RV‚Çú‚Çã‚ÇÇ‚ÇÇ:‚Çú‚Çã‚ÇÅ: Monthly component (22-day average)\n",
        "\n",
        "**3. HAR-X Model**\n",
        "\n",
        "Extends HAR by adding exogenous variables:\n",
        "\n",
        "log(RV‚Çú) = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑RV‚Çú‚Çã‚ÇÅ + Œ≤‚ÇÇ¬∑RV‚Çú‚Çã‚ÇÖ:‚Çú‚Çã‚ÇÅ + Œ≤‚ÇÉ¬∑RV‚Çú‚Çã‚ÇÇ‚ÇÇ:‚Çú‚Çã‚ÇÅ + Œ£Œ≥·µ¢¬∑X·µ¢‚Çú + Œµ‚Çú\n",
        "\n",
        "Exogenous variables (X·µ¢‚Çú):\n",
        "- UST10Y (10-Year Treasury Yield)\n",
        "- HYOAS (High Yield Spread)\n",
        "- TermSpread (10Y-2Y)\n",
        "- VIX (Volatility Index)\n",
        "- Breakeven10Y (Inflation expectations)\n",
        "\n",
        "**4. Rolling Window Estimation**\n",
        "\n",
        "Models estimated using rolling windows: 252, 504, 756, 1008, 1260 days\n",
        "\n",
        "**5. Ensemble Forecasting**\n",
        "\n",
        "Predictions combined using inverse QLIKE weighting:\n",
        "\n",
        "w·µ¢ = (1/QLIKE·µ¢) / Œ£‚±º(1/QLIKE‚±º)\n",
        "\n",
        "Final forecast: ≈∑‚Çú = Œ£·µ¢ w·µ¢ √ó ≈∑·µ¢‚Çú\n",
        "\n",
        "**6. Evaluation Metrics**\n",
        "\n",
        "- **QLIKE**: log(œÉÃÇ¬≤‚Çú) + œÉ¬≤‚Çú/œÉÃÇ¬≤‚Çú (forecast calibration)\n",
        "- **MSPE**: ((œÉ¬≤‚Çú - œÉÃÇ¬≤‚Çú)/œÉ¬≤‚Çú)¬≤ (percentage error)\n",
        "- **RMSE**: ‚àö(E[(œÉ¬≤‚Çú - œÉÃÇ¬≤‚Çú)¬≤]) (absolute error)\n",
        "- **Diebold-Mariano Test**: Statistical comparison of forecast accuracy\n",
        "- **Ljung-Box Test**: Residual autocorrelation check\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úì Introduction sections added to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eb2a8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "40eb2a8a",
        "outputId": "752ee26e-0a3e-4b31-b752-559f210152d9"
      },
      "outputs": [],
      "source": [
        "# diagnotics check before HAR modelling\n",
        "summary_rows = []\n",
        "for col in vol_estimator_check.columns:\n",
        "    res = vol_check.summarize_series(vol_estimator_check[col], name=col)\n",
        "    summary_rows.append({\n",
        "        \"Estimator\": col,\n",
        "        \"ADF stat\": res.get(\"adf_stat\"),\n",
        "        \"ADF p\": res.get(\"adf_p\"),\n",
        "        \"ADF pass (p‚â§Œ±)\": res.get(\"adf_p\") is not None and res[\"adf_p\"] <= vol_check.alpha,\n",
        "        \"KPSS stat\": res.get(\"kpss_stat\"),\n",
        "        \"KPSS p\": res.get(\"kpss_p\"),\n",
        "        \"KPSS pass (p>Œ±)\": res.get(\"kpss_p\") is not None and res[\"kpss_p\"] > vol_check.alpha,\n",
        "        \"LB p @10\": res.get(\"lb_lb_p_10\"),\n",
        "        \"LB p @20\": res.get(\"lb_lb_p_20\"),\n",
        "        \"White noise (LB)\": res.get(\"lb_white_noise_flag\"),\n",
        "    })\n",
        "\n",
        "diag_tbl = pd.DataFrame(summary_rows).set_index(\"Estimator\")\n",
        "\n",
        "# Convenience column: both stationarity tests agree\n",
        "diag_tbl[\"Stationary (ADF‚à©KPSS)\"] = diag_tbl[\"ADF pass (p‚â§Œ±)\"] & diag_tbl[\"KPSS pass (p>Œ±)\"]\n",
        "\n",
        "with pd.option_context('display.float_format', lambda v: f\"{v:.4g}\"):\n",
        "    display(diag_tbl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79ce29c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add diagnostics table to report\n",
        "report.add_section(\"Volatility Estimators Analysis\", level=2)\n",
        "report.add_section(\"Pre-Model Diagnostics\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The following table presents the stationarity and autocorrelation diagnostics for each volatility estimator.\n",
        "We use the Augmented Dickey-Fuller (ADF) test and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test to assess stationarity,\n",
        "and the Ljung-Box test to check for serial correlation in the residuals.\n",
        "\"\"\")\n",
        "report.add_table(diag_tbl, caption=\"Table 1: Diagnostic Tests for Volatility Estimators\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa77b8be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fa77b8be",
        "outputId": "6e84ec85-b29b-47d7-c9fe-77cc9ff6dfde"
      },
      "outputs": [],
      "source": [
        "# Plot ACF and PACF for each log-vol estimator\n",
        "for col in vol_estimator_check.columns:\n",
        "    print(f\"=== {col} ===\")\n",
        "    vol_check.plot_acf(vol_estimator_check[col], nlags=40, title=f\"ACF - {col}\")\n",
        "    vol_check.plot_pacf(vol_estimator_check[col], nlags=40, title=f\"PACF - {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f83c09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save ACF and PACF plots to report\n",
        "report.add_section(\"ACF and PACF Analysis\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots reveal:\n",
        "- **Slow decay in ACF**: Indicates long memory in volatility, consistent with volatility clustering\n",
        "- **Significant PACF spikes**: Suggests short-term AR effects up to 5-15 lags\n",
        "- **HAR model justification**: These patterns support using daily (1), weekly (5), and monthly (22) lags\n",
        "\"\"\")\n",
        "\n",
        "for col in vol_estimator_check.columns:\n",
        "    # ACF plot\n",
        "    fig_acf = plt.figure(figsize=(12, 4))\n",
        "    plot_acf(vol_estimator_check[col].dropna(), lags=40, ax=plt.gca())\n",
        "    plt.title(f\"ACF - {col}\")\n",
        "    report.save_and_add_plot(fig_acf, f\"acf_{col}\", caption=f\"ACF for {col}\")\n",
        "    plt.close()\n",
        "    \n",
        "    # PACF plot\n",
        "    fig_pacf = plt.figure(figsize=(12, 4))\n",
        "    plot_pacf(vol_estimator_check[col].dropna(), lags=40, ax=plt.gca())\n",
        "    plt.title(f\"PACF - {col}\")\n",
        "    report.save_and_add_plot(fig_pacf, f\"pacf_{col}\", caption=f\"PACF for {col}\")\n",
        "    plt.close()\n",
        "    \n",
        "print(\"‚úì ACF and PACF plots saved to report\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbec79d",
      "metadata": {
        "id": "1fbec79d"
      },
      "source": [
        "### Premodel check on the data\n",
        "- All estimators pass the ADF test, indicating it is mean stationary. Can proceed with HAR model fitting.\n",
        "- HAR model requires the data to be covariance-stationary, which your ADF result already supports.\n",
        "- All fail the KPSS test, suggesting that there is trend-stationary / near-unit-root behaviour, which is expected in volatility data case.\n",
        "- All ACF shows slow decay, indicating long memory.\n",
        "- PACF has significant spikes up to ~5‚Äì15 lags ‚Üí short-term AR effects + persistent long-term influence.\n",
        "\n",
        "### What it means in HAR (1,5,22) models?\n",
        "- HAR(1) ‚Üí daily dependence (lag 1)\n",
        "- HAR(5) ‚Üí weekly average dependence (captures medium decay)\n",
        "- HAR(22) ‚Üí monthly average dependence (captures long tail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GQ2_6P0JRXoD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "GQ2_6P0JRXoD",
        "outputId": "7548b27c-90c4-4fbe-a0b2-d402602ffdcd"
      },
      "outputs": [],
      "source": [
        "y_true_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1TTDlyWYReqC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "1TTDlyWYReqC",
        "outputId": "a9ca4624-c414-4815-e974-46df185db95d"
      },
      "outputs": [],
      "source": [
        "y_predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ROLNNNdcRjU5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROLNNNdcRjU5",
        "outputId": "33ff0e49-de2c-4c44-bf8c-bdff0aaa936f"
      },
      "outputs": [],
      "source": [
        "comon_idx = y_true_log.index.intersection(y_predictors.index)\n",
        "y_true_log = y_true_log.loc[comon_idx]\n",
        "y_predictors = y_predictors.loc[comon_idx]\n",
        "print(y_true_log)\n",
        "print(y_predictors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47c6af8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b47c6af8",
        "outputId": "2f4d7703-6645-48c8-af01-596109c90c34"
      },
      "outputs": [],
      "source": [
        "n_total = len(y_predictors)\n",
        "split_point = int(0.7 * n_total)\n",
        "#x_variables\n",
        "train_x = y_predictors.iloc[:split_point]\n",
        "test_x = y_predictors.iloc[split_point:]\n",
        "\n",
        "#y_variables\n",
        "train_y = y_true_log.iloc[:split_point]\n",
        "test_y = y_true_log.iloc[split_point:]\n",
        "\n",
        "print(\"Train X shape:\", train_x.shape)\n",
        "print(\"Test  X shape:\", test_x.shape)\n",
        "print(\"Train y shape:\", train_y.shape)\n",
        "print(\"Test  y shape:\", test_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NOPTN_1mvUGA",
      "metadata": {
        "id": "NOPTN_1mvUGA"
      },
      "outputs": [],
      "source": [
        "window = [252, 504, 756, 1008, 1260]\n",
        "estimators = ['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']\n",
        "per_est = {w: {} for w in window}\n",
        "per_pred = {w: {} for w in window}\n",
        "per_residual = {w: {} for w in window}\n",
        "pred_raw_residual = {w: {} for w in window}\n",
        "\n",
        "df_pred = {}\n",
        "df_pred_adj = {}\n",
        "df_residual = {}\n",
        "df_residual_adj = {}\n",
        "qlike_loss_df = {}\n",
        "mspe_loss_df = {}\n",
        "yhat_var = {}\n",
        "summary_df = {}\n",
        "ljung_box_df = {}\n",
        "\n",
        "\n",
        "for w in window:\n",
        "\n",
        "  for est in estimators:\n",
        "    har = HAR_Model(y_log_col=est, exo_col=None)\n",
        "    x_est = har.features(train_x)\n",
        "    y_adj = train_y.loc[x_est.index] # log variance\n",
        "    per_est[w][est] = x_est\n",
        "\n",
        "    y_pred, resid_pred, residual_raw = har.fit_predict(x_est ,y_adj, window=w)\n",
        "\n",
        "    per_pred[w][est] = y_pred\n",
        "    per_residual[w][est] = resid_pred\n",
        "    pred_raw_residual[w][est] = residual_raw\n",
        "\n",
        "  df_pred[w] = pd.DataFrame(per_pred[w])\n",
        "  df_pred_adj[w] = df_pred[w].dropna()\n",
        "  df_residual[w] = pd.DataFrame(pred_raw_residual[w])\n",
        "  df_residual_adj[w] = df_residual[w].dropna()\n",
        "  residual_input = df_residual_adj[w]\n",
        "\n",
        "  #variance scale\n",
        "  yhat_var[w] = np.exp(df_pred_adj[w])\n",
        "  ytrue_var = np.exp(train_y) #variance scale\n",
        "  common_idx = yhat_var[w].index.intersection(ytrue_var.index)\n",
        "  yhat = yhat_var[w].loc[common_idx]\n",
        "  ytrue = ytrue_var.loc[common_idx]\n",
        "\n",
        "  qlike_loss_df[w] = pd.DataFrame({col: Metric_Evaluation.qlike(ytrue, yhat[col])\n",
        "                                for col in yhat.columns})\n",
        "  mspe_loss_df[w]  = pd.DataFrame({col: Metric_Evaluation.mspe(ytrue, yhat[col])\n",
        "                                for col in yhat.columns})\n",
        "  summary_df[w] = pd.DataFrame({\n",
        "    'QLIKE_mean': qlike_loss_df[w].mean(),\n",
        "    'QLIKE_std':  qlike_loss_df[w].std(),\n",
        "    'MSPE_mean':  mspe_loss_df[w].mean(),\n",
        "    'MSPE_std':   mspe_loss_df[w].std()\n",
        "  }).round(4)\n",
        "\n",
        "  vol_check = Vol_Est_Check(\n",
        "      alpha=0.05,\n",
        "      lb_lags=(10, 20),\n",
        "      kpss_reg='c',\n",
        "      kpss_nlags='auto',\n",
        "      acf_pacf_nlags=40\n",
        "  )\n",
        "  ljung_box_df[w] = pd.DataFrame({col: vol_check.ljung_box(residual_input[col])\n",
        "                              for col in residual_input.columns})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68f6c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf68f6c2",
        "outputId": "069b116e-ef06-41c5-886d-bb6367074db7"
      },
      "outputs": [],
      "source": [
        "final_summary = pd.concat(summary_df, axis=0)\n",
        "final_summary.index.name = 'Window'\n",
        "\n",
        "ljung_box_summary = pd.concat(ljung_box_df, axis=0)\n",
        "ljung_box_summary.index.name = 'Window'\n",
        "\n",
        "print(final_summary)\n",
        "print(ljung_box_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83204d19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HAR model results to report\n",
        "report.add_section(\"HAR Model Results\", level=2)\n",
        "report.add_section(\"Performance Metrics Across Windows\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The HAR model was evaluated across multiple rolling window sizes: 252, 504, 756, 1008, and 1260 days.\n",
        "Below are the comprehensive performance metrics for each estimator and window size.\n",
        "\"\"\")\n",
        "report.add_table(final_summary, caption=\"Table 2: HAR Model Performance Summary (QLIKE and MSPE)\")\n",
        "report.add_table(ljung_box_summary, caption=\"Table 3: Ljung-Box Test Results for HAR Model Residuals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2uKytuo_I7Ya",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2uKytuo_I7Ya",
        "outputId": "12b5f675-3133-4ed8-d40b-dbbbd4d14503"
      },
      "outputs": [],
      "source": [
        "# plot log variance scale\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "\n",
        "for w in window:\n",
        "  common_idx = df_pred_adj[w].index.intersection( y_adj.index)\n",
        "  yhat_plot = df_pred_adj[w].loc[common_idx]\n",
        "  yhat_plot.columns = [f\"{col}_pred\" for col in yhat_plot.columns]\n",
        "\n",
        "  ytrue_plot = train_y.loc[common_idx].to_frame(name = 'true_RV')\n",
        "\n",
        "  plt.figure(figsize=[16,7])\n",
        "  yhat_plot.plot(ax=plt.gca(), alpha=0.9)\n",
        "  ytrue_plot.plot(ax=plt.gca(), color='black', linewidth=2, alpha=0.3, label='True RV')\n",
        "\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Log variance\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fad2f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save HAR prediction plots to report\n",
        "report.add_section(\"HAR Model Predictions vs True RV\", level=3)\n",
        "report.add_text(\"The following plots compare the predicted volatility from each estimator against the true realized volatility.\")\n",
        "\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "for w in window:\n",
        "    common_idx = df_pred_adj[w].index.intersection(y_adj.index)\n",
        "    yhat_plot = df_pred_adj[w].loc[common_idx]\n",
        "    yhat_plot.columns = [f\"{col}_pred\" for col in yhat_plot.columns]\n",
        "    ytrue_plot = train_y.loc[common_idx].to_frame(name='true_RV')\n",
        "    \n",
        "    fig = plt.figure(figsize=[16,7])\n",
        "    yhat_plot.plot(ax=plt.gca(), alpha=0.9)\n",
        "    ytrue_plot.plot(ax=plt.gca(), color='black', linewidth=2, alpha=0.3, label='True RV')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Log variance\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    report.save_and_add_plot(fig, f\"har_prediction_w{w}\", \n",
        "                            caption=f\"HAR Model: Predictions vs True RV (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"‚úì HAR prediction plots saved to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDeyzPsuqoRx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bDeyzPsuqoRx",
        "outputId": "874c08e5-3b1f-48f7-cd37-5bc459f684a6"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daeb07d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daeb07d3",
        "outputId": "ce586a18-6258-41ca-c088-6944be7862aa"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log']].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4380034d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save QLIKE and MSPE loss plots to report\n",
        "report.add_section(\"Loss Metrics Over Time\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "QLIKE (Quasi-Likelihood) and MSPE (Mean Squared Prediction Error) are computed over time for each window.\n",
        "These metrics help assess forecast calibration and error magnitude.\n",
        "\"\"\")\n",
        "\n",
        "# QLIKE plots\n",
        "for w in window:\n",
        "    fig = plt.figure(figsize=[16,7])\n",
        "    qlike_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log']].plot(ax=plt.gca())\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"QLIKE\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"QLIKE Loss for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"qlike_loss_w{w}\", caption=f\"QLIKE Loss Over Time (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "# MSPE plots\n",
        "for w in window:\n",
        "    fig = plt.figure(figsize=[16,7])\n",
        "    mspe_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']].plot(ax=plt.gca())\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"MSPE\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"MSPE Loss for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"mspe_loss_w{w}\", caption=f\"MSPE Loss Over Time (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"‚úì Loss metric plots saved to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0X81UutkqL1x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0X81UutkqL1x",
        "outputId": "f0f32c21-e080-4a02-cdd3-110b277ebc86"
      },
      "outputs": [],
      "source": [
        "#in variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  mspe_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"MSPE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"MSPE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N5HsMJVeuz1A",
      "metadata": {
        "id": "N5HsMJVeuz1A"
      },
      "source": [
        "### Findings:\n",
        "- RS estimator perform the worst for all windows. Henec RS estimator is removed from the ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937a6a53",
      "metadata": {
        "id": "937a6a53"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "lgqkfKJzFT0r",
      "metadata": {
        "id": "lgqkfKJzFT0r"
      },
      "source": [
        "### Ensemble Model - Train/Val Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QV0bGRScqLx5",
      "metadata": {
        "id": "QV0bGRScqLx5"
      },
      "outputs": [],
      "source": [
        "# creating ensemble model for all 5 windows\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "\n",
        "qlike_ensemble = {}\n",
        "wts = {}\n",
        "weight_ensemble = {}\n",
        "yhat_ensemble = {}\n",
        "yhat_enfinal = {}\n",
        "log_yhat_enfinal = {}\n",
        "log_yhat_ensemble = {}\n",
        "residual_ensemble = {}\n",
        "qlike_loss_ensemble = {}\n",
        "mspe_loss_ensemble = {}\n",
        "summary_ensemble = {}\n",
        "ljung_box_ensemble = {}\n",
        "\n",
        "for w in window:\n",
        "\n",
        "  #compute weightage\n",
        "  ensemble_model = EnsembleModel(estimators=None)\n",
        "  qlike_ensemble[w] = summary_df[w]['QLIKE_mean']\n",
        "  weight_ensemble[w] = ensemble_model.compute_weightage(qlike_ensemble[w])\n",
        "  yhat_ensemble[w] = (np.exp(df_pred_adj[w]))\n",
        "\n",
        "  wts[w] = pd.Series(weight_ensemble[w], index=yhat_ensemble[w].columns, dtype=float)\n",
        "\n",
        "  yhat_enfinal[w] = yhat_ensemble[w].dot(wts[w])\n",
        "  log_yhat_enfinal[w] = np.log(  yhat_enfinal[w])\n",
        "\n",
        "  common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "  log_yhat_ensemble[w] = log_yhat_enfinal[w].loc[common_idx] #log-variance\n",
        "  log_ytrue_ensemble = y_adj.loc[common_idx] #log-variance\n",
        "  ytrue_ensemble = ytrue_var.loc[common_idx] # variance\n",
        "\n",
        "  residual_ensemble[w] = log_yhat_ensemble[w] - log_ytrue_ensemble\n",
        "\n",
        "  qlike_loss_ensemble[w] = pd.DataFrame(Metric_Evaluation.qlike(ytrue_ensemble, yhat_enfinal[w]))\n",
        "  mspe_loss_ensemble[w]  = pd.DataFrame(Metric_Evaluation.mspe(ytrue_ensemble, yhat_enfinal[w]))\n",
        "\n",
        "  summary_ensemble[w] = pd.DataFrame({\n",
        "    'QLIKE_mean': qlike_loss_ensemble[w].mean(),\n",
        "    'QLIKE_std':  qlike_loss_ensemble[w].std(),\n",
        "    'MSPE_mean':  mspe_loss_ensemble[w].mean(),\n",
        "    'MSPE_std':   mspe_loss_ensemble[w].std()\n",
        "  }).round(4)\n",
        "\n",
        "  vol_check = Vol_Est_Check(\n",
        "      alpha=0.05,\n",
        "      lb_lags=(10, 20),\n",
        "      kpss_reg='c',\n",
        "      kpss_nlags='auto',\n",
        "      acf_pacf_nlags=40\n",
        "  )\n",
        "  ljung_box_ensemble[w] = pd.DataFrame(vol_check.ljung_box(residual_ensemble[w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tVCuS7iQUnfI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVCuS7iQUnfI",
        "outputId": "8dd0ddda-13ef-4635-8696-38522abe9e8a"
      },
      "outputs": [],
      "source": [
        "for w in window:\n",
        "    print(w, wts[w].round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MrWI2SmdAa6l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrWI2SmdAa6l",
        "outputId": "f28466b5-1976-4b12-b954-a6cab87fc013"
      },
      "outputs": [],
      "source": [
        "final_summary_ensemble = pd.concat(summary_ensemble, axis=0)\n",
        "final_summary_ensemble.index.name = 'Window'\n",
        "\n",
        "lb_ensemble_final = pd.concat(ljung_box_ensemble, axis=0)\n",
        "lb_ensemble_final.index.name = 'Window'\n",
        "\n",
        "print(final_summary_ensemble)\n",
        "print(lb_ensemble_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c17c972",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add ensemble model results to report\n",
        "report.add_section(\"Ensemble Model Results\", level=2)\n",
        "report.add_section(\"Ensemble Weights\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The ensemble model combines predictions from multiple estimators using inverse QLIKE weighting.\n",
        "Below are the weights assigned to each estimator for different window sizes.\n",
        "\"\"\")\n",
        "\n",
        "weights_df = pd.DataFrame({w: wts[w] for w in window}).T\n",
        "weights_df.index.name = 'Window'\n",
        "report.add_table(weights_df.round(4), caption=\"Table 4: Ensemble Model Weights by Window\")\n",
        "\n",
        "report.add_section(\"Ensemble Performance Summary\", level=3)\n",
        "report.add_table(final_summary_ensemble, caption=\"Table 5: Ensemble Model Performance Metrics\")\n",
        "report.add_table(lb_ensemble_final, caption=\"Table 6: Ensemble Model Ljung-Box Test Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MXE-f5gO0SAn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MXE-f5gO0SAn",
        "outputId": "e46cfe70-d4e6-466f-d48f-b327aabda5dc"
      },
      "outputs": [],
      "source": [
        "# plot log variance scale\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "for w in window:\n",
        "  common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "  yhat_plot = log_yhat_enfinal[w].loc[common_idx].to_frame(name = 'Ensemble_RV') #log-variance\n",
        "  ytrue_plot = y_adj.loc[common_idx].to_frame(name = 'true_RV') #log-variance\n",
        "\n",
        "  y_plot = pd.concat([yhat_plot, ytrue_plot], axis = 1)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(16, 7))\n",
        "  yhat_plot.plot(ax=ax, color='blue', linewidth=2, label='Ensemble_RV')\n",
        "  ytrue_plot.plot(ax=ax, color='orange', linewidth=1.5, alpha=0.5, label='true_RV')\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Log variance\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XrutlNOTSa0p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XrutlNOTSa0p",
        "outputId": "387329a5-7b49-489e-f3eb-ae9ec9b9f2fc"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_ensemble[w].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eL4eWgsXR8tI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eL4eWgsXR8tI",
        "outputId": "12aaab35-1f0a-4685-86a2-fb003c443f7a"
      },
      "outputs": [],
      "source": [
        "#in variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  mspe_loss_ensemble[w].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"MSPE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"MSPE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3760e9f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save ensemble plots to report\n",
        "report.add_section(\"Ensemble Predictions and Loss Metrics\", level=3)\n",
        "\n",
        "# Ensemble predictions\n",
        "for w in window:\n",
        "    common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "    yhat_plot = log_yhat_enfinal[w].loc[common_idx].to_frame(name='Ensemble_RV')\n",
        "    ytrue_plot = y_adj.loc[common_idx].to_frame(name='true_RV')\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 7))\n",
        "    yhat_plot.plot(ax=ax, color='blue', linewidth=2, label='Ensemble_RV')\n",
        "    ytrue_plot.plot(ax=ax, color='orange', linewidth=1.5, alpha=0.5, label='true_RV')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Log variance\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"Ensemble HAR prediction vs true RV for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"ensemble_pred_w{w}\", \n",
        "                            caption=f\"Ensemble Model: Predictions vs True RV (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "# Ensemble QLIKE\n",
        "for w in window:\n",
        "    fig = plt.figure(figsize=[16,7])\n",
        "    qlike_loss_ensemble[w].plot(ax=plt.gca())\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"QLIKE\")\n",
        "    plt.title(f\"QLIKE Loss for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"ensemble_qlike_w{w}\", \n",
        "                            caption=f\"Ensemble QLIKE Loss (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "# Ensemble MSPE\n",
        "for w in window:\n",
        "    fig = plt.figure(figsize=[16,7])\n",
        "    mspe_loss_ensemble[w].plot(ax=plt.gca())\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"MSPE\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"MSPE Loss for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"ensemble_mspe_w{w}\", \n",
        "                            caption=f\"Ensemble MSPE Loss (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"‚úì Ensemble plots saved to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WFNF81TgT_Oo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WFNF81TgT_Oo",
        "outputId": "ea02e8bf-76b8-4d83-b912-dd4884a09d9e"
      },
      "outputs": [],
      "source": [
        "# plot acf and pacf\n",
        "for w in window:\n",
        "    print(f\"=== {w} ===\")\n",
        "    vol_check.plot_acf(residual_ensemble[w], nlags=40, title=f\"ACF - Window {w}\")\n",
        "    vol_check.plot_pacf(residual_ensemble[w], nlags=40, title=f\"PACF - Window {w}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ef1ee9",
      "metadata": {
        "id": "03ef1ee9"
      },
      "source": [
        "## Evaluation from the ensemble model\n",
        "- Windows = [252, 504, 756, 1008, 1260]\n",
        "- Overall forecast calibration (QLIKE) : 252 perform best\n",
        "- Raw forecast error magnitude (MSPE): 504 perform best\n",
        "- Ljung box test: All windows passed with 756/1008 windows showing the highest p-values, indicating lesser degree of autocorrelation\n",
        "- PACF and ACF plots for all windows: No significant autocorrelataion beyond lag 0. Residuals behave like white noise. Indication of the ability to capture the volatility dynamics.\n",
        "- Hence windows 504 and 756 will run through the DM test to check for pairwise statistical validation (Test whether the difference in predictive loss is statistically significant)\n",
        "\n",
        "- Ensemble generally outperforms individual estimators in QLIKE, MSPE, volatility/stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83aaa67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83aaa67",
        "outputId": "91d305d5-a325-49c2-f66f-10e0409f5f13"
      },
      "outputs": [],
      "source": [
        "#window = [504, 756]\n",
        "loss1 = qlike_loss_ensemble[504]\n",
        "loss2 = qlike_loss_ensemble[756]\n",
        "common_idx = loss2.index.intersection( loss1.index)\n",
        "loss2_adj = loss2.loc[common_idx]\n",
        "loss1_adj = loss1.loc[common_idx]\n",
        "\n",
        "DM_test_results = Metric_Evaluation.DM_test(loss1_adj,\n",
        "                                            loss2_adj,\n",
        "                                            model1_name='Window_504',\n",
        "                                            model2_name='Window_756'\n",
        "                                            )\n",
        "print(DM_test_results)\n",
        "\n",
        "# p value = 0.8222 >0.05, fail to reject Ho of equal predictive accuracy.\n",
        "#indicates no statistically significant difference in predictive accuracy between window 504 and 756.\n",
        "# Therefore, either window may be used, and selection can be based on secondary metrics or practical considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196629d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add DM test results to report\n",
        "report.add_section(\"Diebold-Mariano Test\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The Diebold-Mariano (DM) test evaluates whether there is a statistically significant difference \n",
        "in predictive accuracy between two models. Here we compare Windows 504 and 756.\n",
        "\"\"\")\n",
        "\n",
        "dm_stat, p_val, decision = DM_test_results\n",
        "dm_results = {\n",
        "    \"DM Statistic\": dm_stat,\n",
        "    \"P-value\": p_val,\n",
        "    \"Better Model\": decision['Better model'],\n",
        "    \"Significant?\": decision['Significant'],\n",
        "    \"Alpha\": decision['Alpha'],\n",
        "    \"Observations\": decision['Observations']\n",
        "}\n",
        "report.add_metrics_summary(dm_results, title=\"DM Test Results: Window 504 vs Window 756\")\n",
        "\n",
        "report.add_text(f\"\"\"\n",
        "**Interpretation:** With p-value = {p_val:.4f} > 0.05, we fail to reject the null hypothesis of equal \n",
        "predictive accuracy. This indicates no statistically significant difference between Window 504 and 756.\n",
        "Either window may be used, and selection can be based on secondary metrics or practical considerations.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c2edef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98c2edef",
        "outputId": "15a88088-2729-4926-9da4-f73f8ae0141c"
      },
      "outputs": [],
      "source": [
        "for w in window:\n",
        "    print(f'\\n Window {w}')\n",
        "    a = qlike_loss_ensemble[w].describe()\n",
        "    print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FapVXZDqGoWk",
      "metadata": {
        "id": "FapVXZDqGoWk"
      },
      "source": [
        "## HAR-X model\n",
        "- include other variables apart from the historical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KM3x9DQuqLuQ",
      "metadata": {
        "id": "KM3x9DQuqLuQ"
      },
      "outputs": [],
      "source": [
        "# Read into DataFrame - exogeneous variables\n",
        "IV_y_values = pd.read_csv('MOVE_index.csv')\n",
        "Fed_funds = pd.read_csv('FedFunds.csv')\n",
        "UST_10Y = pd.read_csv('UST10Y.csv')\n",
        "HYOAS = pd.read_csv('HYOAS.csv')\n",
        "NFCI = pd.read_csv('NFCI.csv')\n",
        "Termspread = pd.read_csv('TermSpread_10Y_2Y.csv')\n",
        "vix = pd.read_csv('VIX.csv')\n",
        "Breakeven_10Y = pd.read_csv('Breakeven10Y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VVmpw2Izh-W",
      "metadata": {
        "id": "5VVmpw2Izh-W"
      },
      "outputs": [],
      "source": [
        "exo_variables = [UST_10Y, HYOAS, Termspread, vix, Breakeven_10Y]\n",
        "\n",
        "for i, df in enumerate(exo_variables):\n",
        "  df['Date'] = pd.to_datetime(df['Date'])\n",
        "  df.set_index('Date', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8m_frX2yzZC_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8m_frX2yzZC_",
        "outputId": "8354f5b3-5da7-42d9-9258-80ce26a410ea"
      },
      "outputs": [],
      "source": [
        "Breakeven_10Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fQq8jKkuc7Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0fQq8jKkuc7Y",
        "outputId": "cce2827a-bb04-47cb-be7b-50eefb0684c6"
      },
      "outputs": [],
      "source": [
        "# Fed_funds - monthly data\n",
        "# UST_10Y 5740 data\n",
        "# HYOAS 5814 data\n",
        "# NFCI - weekly data 1148 data\n",
        "# Termspread 5740 data\n",
        "# vix 5740 data\n",
        "# Breakeven_10Y 5739 data\n",
        "\n",
        "exo_variable_all = pd.concat(exo_variables, axis=1, join = 'outer')\n",
        "# axis = 1 to concat column-wise, join = 'outer' keep all dates, join = 'inner' keep common dates\n",
        "\n",
        "exo_variable_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WSLjTcgpuc3n",
      "metadata": {
        "id": "WSLjTcgpuc3n"
      },
      "outputs": [],
      "source": [
        "exo_var_adj = exo_variable_all.copy()\n",
        "exo_var_adj.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qHVOZzQoBW4l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "qHVOZzQoBW4l",
        "outputId": "c24e3f1d-460d-4f12-e6c6-1042a55006c6"
      },
      "outputs": [],
      "source": [
        "y_predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CFD6WdxH1ha2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "CFD6WdxH1ha2",
        "outputId": "a67eb9b0-b6b7-493d-aa20-09b20e3e2d4d"
      },
      "outputs": [],
      "source": [
        "# to do lag1 to prepare for ADF -> modelling\n",
        "master_idx = vol_results_adj.index\n",
        "exo_adj =\\\n",
        "(\n",
        "   exo_var_adj\n",
        "   .reindex(index = master_idx)\n",
        "   .ffill()\n",
        ")\n",
        "exo_adj.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LWWskPamKLr7",
      "metadata": {
        "id": "LWWskPamKLr7"
      },
      "outputs": [],
      "source": [
        "# standardization using expanding window to prevent look ahead bias\n",
        "\n",
        "exo_label = ['UST10Y', 'HYOAS', 'TermSpread_10Y_2Y', 'VIX', 'Breakeven10Y']\n",
        "\n",
        "def Stdize_ExoVariables(df):\n",
        "  df = df.copy()\n",
        "  out = pd.DataFrame(index = df.index)\n",
        "\n",
        "  for exo in exo_label:\n",
        "        mean_series = df[exo].expanding().mean().shift(1)\n",
        "        std_series  = df[exo].expanding().std(ddof=1).shift(1)\n",
        "        z_series = (df[exo] - mean_series) / std_series\n",
        "\n",
        "        out[f'std_mean_{exo}'] = mean_series\n",
        "        out[f'std_dev_{exo}'] = std_series\n",
        "        out[f'{exo}'] = z_series\n",
        "\n",
        "  return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uW-NbO5HapTg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "uW-NbO5HapTg",
        "outputId": "beee8a0a-7ae0-4367-c6ae-29da7d4b255d"
      },
      "outputs": [],
      "source": [
        "exo_std_df = Stdize_ExoVariables(exo_adj)\n",
        "exo_std_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dUcUz-I-1hRP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "dUcUz-I-1hRP",
        "outputId": "f506327f-a605-468f-8468-29e78e1e04c4"
      },
      "outputs": [],
      "source": [
        "exo_std_df = exo_std_df.dropna()\n",
        "exo_label = ['UST10Y', 'HYOAS', 'TermSpread_10Y_2Y', 'VIX', 'Breakeven10Y']\n",
        "exo_std_harx = exo_std_df[exo_label]\n",
        "exo_std_harx_adj = exo_std_harx.loc[:'2024-12-27']\n",
        "exo_std_harx_adj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wU1KyZaDdNDw",
      "metadata": {
        "id": "wU1KyZaDdNDw"
      },
      "outputs": [],
      "source": [
        "vol_adj_harx = y_predictors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E9hPnI3O0FaW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "E9hPnI3O0FaW",
        "outputId": "db21b4d9-babe-45cc-81b4-2f683f9987d6"
      },
      "outputs": [],
      "source": [
        "vol_check = Vol_Est_Check(\n",
        "    alpha=0.05,\n",
        "    lb_lags=(10, 20),\n",
        "    kpss_reg='c',\n",
        "    kpss_nlags='auto',\n",
        "    acf_pacf_nlags=40\n",
        ")\n",
        "\n",
        "for col in exo_std_harx_adj.columns:\n",
        "    print(f\"=== Diagnostics for {col} ===\")\n",
        "    result = vol_check.summarize_series(exo_std_harx_adj[col], name=col)\n",
        "    print(result, \"\\n\")\n",
        "\n",
        "# diagnotics check before HAR modelling\n",
        "summary_rows = []\n",
        "for col in exo_std_harx_adj.columns:\n",
        "    res = vol_check.summarize_series(exo_std_harx_adj[col], name=col)\n",
        "    summary_rows.append({\n",
        "        \"Estimator\": col,\n",
        "        \"ADF stat\": res.get(\"adf_stat\"),\n",
        "        \"ADF p\": res.get(\"adf_p\"),\n",
        "        \"ADF pass (p‚â§Œ±)\": res.get(\"adf_p\") is not None and res[\"adf_p\"] <= vol_check.alpha,\n",
        "        \"KPSS stat\": res.get(\"kpss_stat\"),\n",
        "        \"KPSS p\": res.get(\"kpss_p\"),\n",
        "        \"KPSS pass (p>Œ±)\": res.get(\"kpss_p\") is not None and res[\"kpss_p\"] > vol_check.alpha,\n",
        "        \"LB p @10\": res.get(\"lb_lb_p_10\"),\n",
        "        \"LB p @20\": res.get(\"lb_lb_p_20\"),\n",
        "        \"White noise (LB)\": res.get(\"lb_white_noise_flag\"),\n",
        "    })\n",
        "\n",
        "diag_tbl = pd.DataFrame(summary_rows).set_index(\"Estimator\")\n",
        "\n",
        "# Convenience column: both stationarity tests agree\n",
        "diag_tbl[\"Stationary (ADF‚à©KPSS)\"] = diag_tbl[\"ADF pass (p‚â§Œ±)\"] & diag_tbl[\"KPSS pass (p>Œ±)\"]\n",
        "\n",
        "with pd.option_context('display.float_format', lambda v: f\"{v:.4g}\"):\n",
        "    display(diag_tbl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c975e5e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HARX exogenous variables diagnostics to report\n",
        "report.add_section(\"HAR-X Model Results\", level=2)\n",
        "report.add_section(\"Exogenous Variables\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The HAR-X model extends the HAR model by incorporating exogenous variables:\n",
        "- **UST10Y**: 10-Year US Treasury Yield\n",
        "- **HYOAS**: High Yield Option-Adjusted Spread\n",
        "- **TermSpread_10Y_2Y**: Term Spread (10Y - 2Y)\n",
        "- **VIX**: CBOE Volatility Index\n",
        "- **Breakeven10Y**: 10-Year Breakeven Inflation Rate\n",
        "\n",
        "All exogenous variables were standardized using expanding window standardization to prevent look-ahead bias.\n",
        "\"\"\")\n",
        "\n",
        "report.add_table(diag_tbl, caption=\"Table 7: Diagnostic Tests for Exogenous Variables (After Differencing)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KQeovBKnf0lC",
      "metadata": {
        "id": "KQeovBKnf0lC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0h5L2gy7fW6M",
      "metadata": {
        "id": "0h5L2gy7fW6M"
      },
      "source": [
        "## First run of ADF test on exogeneoous variables\n",
        "- TermSpread_10Y_2Y passed the test. Hence it will do differencing to remove the trend aspect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLpMQkuVYrSa",
      "metadata": {
        "id": "wLpMQkuVYrSa"
      },
      "outputs": [],
      "source": [
        "exo_std_harx_r1 = exo_std_harx_adj.copy()\n",
        "exo_std_harx_r1['TermSpread_10Y_2Y'] = exo_std_harx_r1['TermSpread_10Y_2Y'].diff()\n",
        "exo_std_harx_r1 = exo_std_harx_r1.dropna()\n",
        "\n",
        "# vol_adj_harx = y_predictors.loc['2003-01-08':]\n",
        "\n",
        "# print(vol_adj_harx)\n",
        "\n",
        "# print(exo_std_harx_r1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LGfPo5UKTOxw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGfPo5UKTOxw",
        "outputId": "6f140721-1698-4b05-ef54-c63ecc919a1a"
      },
      "outputs": [],
      "source": [
        "# keep only common index between the frames\n",
        "common_idx = vol_adj_harx.index.intersection(exo_std_harx_r1.index)\n",
        "\n",
        "vol_adj_harx = vol_adj_harx.loc[common_idx]\n",
        "exo_std_harx_r1 =exo_std_harx_r1.loc[common_idx]\n",
        "\n",
        "print(vol_adj_harx )\n",
        "print(exo_std_harx_r1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UV_-jnUmYrPQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "UV_-jnUmYrPQ",
        "outputId": "9ecfab7c-061e-4af2-e97f-bf897945f297"
      },
      "outputs": [],
      "source": [
        "for col in exo_std_harx_r1.columns:\n",
        "    print(f\"=== Diagnostics for {col} ===\")\n",
        "    result = vol_check.summarize_series(exo_std_harx_r1[col], name=col)\n",
        "    print(result, \"\\n\")\n",
        "\n",
        "# diagnotics check before HAR modelling\n",
        "summary_rows = []\n",
        "for col in exo_std_harx_r1.columns:\n",
        "    res = vol_check.summarize_series(exo_std_harx_r1[col], name=col)\n",
        "    summary_rows.append({\n",
        "        \"Estimator\": col,\n",
        "        \"ADF stat\": res.get(\"adf_stat\"),\n",
        "        \"ADF p\": res.get(\"adf_p\"),\n",
        "        \"ADF pass (p‚â§Œ±)\": res.get(\"adf_p\") is not None and res[\"adf_p\"] <= vol_check.alpha,\n",
        "        \"KPSS stat\": res.get(\"kpss_stat\"),\n",
        "        \"KPSS p\": res.get(\"kpss_p\"),\n",
        "        \"KPSS pass (p>Œ±)\": res.get(\"kpss_p\") is not None and res[\"kpss_p\"] > vol_check.alpha,\n",
        "        \"LB p @10\": res.get(\"lb_lb_p_10\"),\n",
        "        \"LB p @20\": res.get(\"lb_lb_p_20\"),\n",
        "        \"White noise (LB)\": res.get(\"lb_white_noise_flag\"),\n",
        "    })\n",
        "\n",
        "diag_tbl = pd.DataFrame(summary_rows).set_index(\"Estimator\")\n",
        "\n",
        "# Convenience column: both stationarity tests agree\n",
        "diag_tbl[\"Stationary (ADF‚à©KPSS)\"] = diag_tbl[\"ADF pass (p‚â§Œ±)\"] & diag_tbl[\"KPSS pass (p>Œ±)\"]\n",
        "\n",
        "with pd.option_context('display.float_format', lambda v: f\"{v:.4g}\"):\n",
        "    display(diag_tbl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QArg_caKluiX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QArg_caKluiX",
        "outputId": "e721d15c-61d2-428b-bf64-fd6db8ba33d2"
      },
      "outputs": [],
      "source": [
        "# Plot ACF and PACF for each log-vol estimator\n",
        "for col in exo_std_harx_r1.columns:\n",
        "    print(f\"=== {col} ===\")\n",
        "    vol_check.plot_acf(exo_std_harx_r1[col], nlags=40, title=f\"ACF - {col}\")\n",
        "    vol_check.plot_pacf(exo_std_harx_r1[col], nlags=40, title=f\"PACF - {col}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-vOZRf4kFNI",
      "metadata": {
        "id": "k-vOZRf4kFNI"
      },
      "source": [
        "## Adjust exogeneous variables\n",
        "- After the first differencing done on TermSpread_10Y_2Y, all the exogeneous variables passed the ADF test.\n",
        "- TermSpread_10Y_2Y passed the KPSS test.\n",
        "\n",
        "## Proceed to run through the HARX modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pe6Uv2l9lvr2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "pe6Uv2l9lvr2",
        "outputId": "0de9c304-3fe3-4d34-a7e1-28d3ea25ec03"
      },
      "outputs": [],
      "source": [
        "y_true_log_harx = y_true_log.loc[common_idx]\n",
        "y_true_log_harx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JyLU_wJOk_--",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "JyLU_wJOk_--",
        "outputId": "10878fc9-7ab4-418c-9899-e5e2e564976f"
      },
      "outputs": [],
      "source": [
        "# vol_adj_harx\n",
        "# exo_std_harx_r1\n",
        "# y_true_log_harx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o_VHhvK5YrLx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_VHhvK5YrLx",
        "outputId": "1bb05429-bf1b-4c97-ef0b-01a1ca1f3a94"
      },
      "outputs": [],
      "source": [
        "n_total = len(vol_adj_harx)\n",
        "split_point = int(0.7 * n_total)\n",
        "#x_variables\n",
        "train_x = vol_adj_harx.iloc[:split_point]\n",
        "test_x = vol_adj_harx.iloc[split_point:]\n",
        "\n",
        "exo_harx_train = exo_std_harx_r1.iloc[:split_point]\n",
        "exo_harx_test = exo_std_harx_r1.iloc[split_point:]\n",
        "\n",
        "#y_variables\n",
        "train_y = y_true_log_harx.iloc[:split_point]\n",
        "test_y = y_true_log_harx.iloc[split_point:]\n",
        "\n",
        "print(\"Train X shape:\", train_x.shape)\n",
        "print(\"Test  X shape:\", test_x.shape)\n",
        "print(\"Train y shape:\", train_y.shape)\n",
        "print(\"Test  y shape:\", test_y.shape)\n",
        "print('Train Exo shape:' , exo_harx_train.shape)\n",
        "print('Test Exo shape:' , exo_harx_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LD8c4sdfYrDx",
      "metadata": {
        "id": "LD8c4sdfYrDx"
      },
      "outputs": [],
      "source": [
        "window = [252, 504, 756, 1008, 1260]\n",
        "estimators = ['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']\n",
        "per_est = {w: {} for w in window}\n",
        "per_pred = {w: {} for w in window}\n",
        "per_residual = {w: {} for w in window}\n",
        "pred_raw_residual = {w: {} for w in window}\n",
        "\n",
        "df_pred = {}\n",
        "df_pred_adj = {}\n",
        "df_residual = {}\n",
        "df_residual_adj = {}\n",
        "qlike_loss_df = {}\n",
        "mspe_loss_df = {}\n",
        "yhat_var = {}\n",
        "summary_df = {}\n",
        "ljung_box_df = {}\n",
        "\n",
        "exo_cols = ['UST10Y', 'HYOAS', 'TermSpread_10Y_2Y', 'VIX', 'Breakeven10Y']\n",
        "\n",
        "for w in window:\n",
        "\n",
        "  for est in estimators:\n",
        "    df_in = pd.concat([train_x[[est]], exo_std_harx_r1[exo_cols]], axis=1)\n",
        "    har = HAR_Model(y_log_col=est, exo_col=exo_cols, lags=[1,5,22])\n",
        "    x_est = har.features(df_in)\n",
        "    y_adj = train_y.loc[x_est.index]\n",
        "    per_est[w][est] = x_est\n",
        "\n",
        "    y_pred, resid_pred, residual_raw = har.fit_predict(x_est ,y_adj, window=w)\n",
        "\n",
        "    per_pred[w][est] = y_pred\n",
        "    per_residual[w][est] = resid_pred\n",
        "    pred_raw_residual[w][est] = residual_raw\n",
        "\n",
        "  df_pred[w] = pd.DataFrame(per_pred[w])\n",
        "  df_pred_adj[w] = df_pred[w].dropna()\n",
        "  df_residual[w] = pd.DataFrame(pred_raw_residual[w])\n",
        "  df_residual_adj[w] = df_residual[w].dropna()\n",
        "  residual_input = df_residual_adj[w]\n",
        "\n",
        "  #variance scale\n",
        "  yhat_var[w] = np.exp(df_pred_adj[w])\n",
        "  ytrue_var = np.exp(train_y)\n",
        "  common_idx = yhat_var[w].index.intersection(ytrue_var.index)\n",
        "  yhat = yhat_var[w].loc[common_idx]\n",
        "  ytrue = ytrue_var.loc[common_idx]\n",
        "\n",
        "  qlike_loss_df[w] = pd.DataFrame({col: Metric_Evaluation.qlike(ytrue, yhat[col])\n",
        "                                for col in yhat.columns})\n",
        "  mspe_loss_df[w]  = pd.DataFrame({col: Metric_Evaluation.mspe(ytrue, yhat[col])\n",
        "                                for col in yhat.columns})\n",
        "  summary_df[w] = pd.DataFrame({\n",
        "    'QLIKE_mean': qlike_loss_df[w].mean(),\n",
        "    'QLIKE_std':  qlike_loss_df[w].std(),\n",
        "    'MSPE_mean':  mspe_loss_df[w].mean(),\n",
        "    'MSPE_std':   mspe_loss_df[w].std()\n",
        "  }).round(4)\n",
        "\n",
        "  vol_check = Vol_Est_Check(\n",
        "      alpha=0.05,\n",
        "      lb_lags=(10, 20),\n",
        "      kpss_reg='c',\n",
        "      kpss_nlags='auto',\n",
        "      acf_pacf_nlags=40\n",
        "  )\n",
        "  ljung_box_df[w] = pd.DataFrame({col: vol_check.ljung_box(residual_input[col])\n",
        "                              for col in residual_input.columns})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4npm75eIkzL2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4npm75eIkzL2",
        "outputId": "b5211391-5f9e-4c0c-f8ea-78afe1b40033"
      },
      "outputs": [],
      "source": [
        "final_summary = pd.concat(summary_df, axis=0)\n",
        "final_summary.index.name = 'Window'\n",
        "\n",
        "ljung_box_summary = pd.concat(ljung_box_df, axis=0)\n",
        "ljung_box_summary.index.name = 'Window'\n",
        "\n",
        "print(final_summary)\n",
        "print(ljung_box_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a49cc47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HARX model performance to report\n",
        "report.add_section(\"HARX Model Performance\", level=3)\n",
        "report.add_text(\"\"\"\n",
        "The HAR-X model performance across different rolling window sizes is presented below.\n",
        "\"\"\")\n",
        "report.add_table(final_summary, caption=\"Table 8: HAR-X Model Performance Summary\")\n",
        "report.add_table(ljung_box_summary, caption=\"Table 9: HAR-X Model Ljung-Box Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3HsuUXoG1emG",
      "metadata": {
        "id": "3HsuUXoG1emG"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NhpKOjfmyWH1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhpKOjfmyWH1",
        "outputId": "5db945e6-1d9d-49ee-8606-a66cd331febd"
      },
      "outputs": [],
      "source": [
        "# plot log variance scale\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "\n",
        "for w in window:\n",
        "  common_idx = df_pred_adj[w].index.intersection( y_adj.index)\n",
        "  yhat_plot = df_pred_adj[w].loc[common_idx]\n",
        "  yhat_plot.columns = [f\"{col}_pred\" for col in yhat_plot.columns]\n",
        "\n",
        "  ytrue_plot = y_adj.loc[common_idx].to_frame(name = 'true_RV')\n",
        "\n",
        "  plt.figure(figsize=[16,7])\n",
        "  yhat_plot.plot(ax=plt.gca(), alpha=0.9)\n",
        "  ytrue_plot.plot(ax=plt.gca(), color='black', linewidth=2, alpha=0.3, label='True RV')\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Log variance\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clauaK4syWEX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "clauaK4syWEX",
        "outputId": "ebd746db-0581-4cd9-86c3-2afbc1223f6d"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tn8yksssyWAt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tn8yksssyWAt",
        "outputId": "175cd945-fec3-4734-d497-b7857f256eed"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_df[w][['square_est_log', 'parkinson_est_log', 'gk_est_log']].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3LU4oow5tvG",
      "metadata": {
        "id": "c3LU4oow5tvG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "hsrMst595uaU",
      "metadata": {
        "id": "hsrMst595uaU"
      },
      "source": [
        "## Creating ensemble model for HARX model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fW0WBRUjyV8V",
      "metadata": {
        "id": "fW0WBRUjyV8V"
      },
      "outputs": [],
      "source": [
        "# creating ensemble model for all 5 windows\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "\n",
        "qlike_ensemble = {}\n",
        "wts = {}\n",
        "weight_ensemble = {}\n",
        "yhat_ensemble = {}\n",
        "yhat_enfinal = {}\n",
        "log_yhat_enfinal = {}\n",
        "log_yhat_ensemble = {}\n",
        "residual_ensemble = {}\n",
        "qlike_loss_ensemble = {}\n",
        "mspe_loss_ensemble = {}\n",
        "summary_ensemble = {}\n",
        "ljung_box_ensemble = {}\n",
        "\n",
        "for w in window:\n",
        "\n",
        "  #compute weightage\n",
        "  ensemble_model = EnsembleModel(estimators=None)\n",
        "  qlike_ensemble[w] = summary_df[w]['QLIKE_mean']\n",
        "  weight_ensemble[w] = ensemble_model.compute_weightage(qlike_ensemble[w])\n",
        "  yhat_ensemble[w] = (np.exp(df_pred_adj[w]))\n",
        "\n",
        "  wts[w] = pd.Series(weight_ensemble[w], index=yhat_ensemble[w].columns, dtype=float)\n",
        "\n",
        "  yhat_enfinal[w] = yhat_ensemble[w].dot(wts[w])\n",
        "  log_yhat_enfinal[w] = np.log(  yhat_enfinal[w])\n",
        "\n",
        "  common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "  log_yhat_ensemble[w] = log_yhat_enfinal[w].loc[common_idx] #log-variance\n",
        "  log_ytrue_ensemble = y_adj.loc[common_idx] #log-variance\n",
        "  ytrue_ensemble = ytrue_var.loc[common_idx] # variance\n",
        "\n",
        "  residual_ensemble[w] = log_yhat_ensemble[w] - log_ytrue_ensemble\n",
        "\n",
        "  qlike_loss_ensemble[w] = pd.DataFrame(Metric_Evaluation.qlike(ytrue_ensemble, yhat_enfinal[w]))\n",
        "  mspe_loss_ensemble[w]  = pd.DataFrame(Metric_Evaluation.mspe(ytrue_ensemble, yhat_enfinal[w]))\n",
        "\n",
        "  summary_ensemble[w] = pd.DataFrame({\n",
        "    'QLIKE_mean': qlike_loss_ensemble[w].mean(),\n",
        "    'QLIKE_std':  qlike_loss_ensemble[w].std(),\n",
        "    'MSPE_mean':  mspe_loss_ensemble[w].mean(),\n",
        "    'MSPE_std':   mspe_loss_ensemble[w].std()\n",
        "  }).round(4)\n",
        "\n",
        "  vol_check = Vol_Est_Check(\n",
        "      alpha=0.05,\n",
        "      lb_lags=(10, 20),\n",
        "      kpss_reg='c',\n",
        "      kpss_nlags='auto',\n",
        "      acf_pacf_nlags=40\n",
        "  )\n",
        "  ljung_box_ensemble[w] = pd.DataFrame(vol_check.ljung_box(residual_ensemble[w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ulz1Anh55ARt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulz1Anh55ARt",
        "outputId": "e89ed389-96e9-4aa1-c579-ca76cd897491"
      },
      "outputs": [],
      "source": [
        "for w in window:\n",
        "    print(w, wts[w].round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pER14esD5AJy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pER14esD5AJy",
        "outputId": "59851e81-9cb9-4095-8828-0a737fe70095"
      },
      "outputs": [],
      "source": [
        "final_summary_ensemble = pd.concat(summary_ensemble, axis=0)\n",
        "final_summary_ensemble.index.name = 'Window'\n",
        "\n",
        "lb_ensemble_final = pd.concat(ljung_box_ensemble, axis=0)\n",
        "lb_ensemble_final.index.name = 'Window'\n",
        "\n",
        "print(final_summary_ensemble)\n",
        "print(lb_ensemble_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f88da1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HARX ensemble results to report\n",
        "report.add_section(\"HARX Ensemble Model\", level=3)\n",
        "\n",
        "weights_df_harx = pd.DataFrame({w: wts[w] for w in window}).T\n",
        "weights_df_harx.index.name = 'Window'\n",
        "report.add_table(weights_df_harx.round(4), caption=\"Table 10: HARX Ensemble Model Weights\")\n",
        "\n",
        "report.add_table(final_summary_ensemble, caption=\"Table 11: HARX Ensemble Performance Metrics\")\n",
        "report.add_table(lb_ensemble_final, caption=\"Table 12: HARX Ensemble Ljung-Box Test Results\")\n",
        "\n",
        "# Save HARX ensemble plots\n",
        "for w in window:\n",
        "    common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "    yhat_plot = log_yhat_enfinal[w].loc[common_idx].to_frame(name='Ensemble_RV')\n",
        "    ytrue_plot = y_adj.loc[common_idx].to_frame(name='true_RV')\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 7))\n",
        "    yhat_plot.plot(ax=ax, color='blue', linewidth=2, label='Ensemble_RV')\n",
        "    ytrue_plot.plot(ax=ax, color='orange', linewidth=1.5, alpha=0.5, label='true_RV')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Log variance\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"HARX Ensemble prediction vs true RV for window {w}\")\n",
        "    plt.tight_layout()\n",
        "    report.save_and_add_plot(fig, f\"harx_ensemble_pred_w{w}\", \n",
        "                            caption=f\"HARX Ensemble: Predictions vs True RV (Window={w})\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"‚úì HARX ensemble results saved to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ObXJvYkA5ADF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ObXJvYkA5ADF",
        "outputId": "e97d7373-e587-4243-b86d-4d98f3d0f61d"
      },
      "outputs": [],
      "source": [
        "# plot log variance scale\n",
        "window = [252, 504, 756, 1008, 1260]\n",
        "for w in window:\n",
        "  common_idx = log_yhat_enfinal[w].index.intersection(y_adj.index)\n",
        "  yhat_plot = log_yhat_enfinal[w].loc[common_idx].to_frame(name = 'Ensemble_RV') #log-variance\n",
        "  ytrue_plot = y_adj.loc[common_idx].to_frame(name = 'true_RV') #log-variance\n",
        "\n",
        "  y_plot = pd.concat([yhat_plot, ytrue_plot], axis = 1)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(16, 7))\n",
        "  yhat_plot.plot(ax=ax, color='blue', linewidth=2, label='Ensemble_RV')\n",
        "  ytrue_plot.plot(ax=ax, color='orange', linewidth=1.5, alpha=0.5, label='true_RV')\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Log variance\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4k4rxEG5kzDf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4k4rxEG5kzDf",
        "outputId": "95ff51bb-f5e7-4633-b78a-34da83fe0353"
      },
      "outputs": [],
      "source": [
        "# IN variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  qlike_loss_ensemble[w].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"QLIKE\")\n",
        "  plt.title(f\"QLIKE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3L-49Lg6gVG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e3L-49Lg6gVG",
        "outputId": "4f62d601-75d6-48d4-e381-a68e59ef8ac4"
      },
      "outputs": [],
      "source": [
        "#in variance scale\n",
        "for w in window:\n",
        "  plt.figure(figsize=[16,7])\n",
        "  mspe_loss_ensemble[w].plot()\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"MSPE\")\n",
        "  plt.legend()\n",
        "  plt.title(f\"MSPE Loss for window {w}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BK7Ksfg6gJn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-BK7Ksfg6gJn",
        "outputId": "2f299d48-d202-4058-cdb7-11b9d9ec925d"
      },
      "outputs": [],
      "source": [
        "# plot acf and pacf\n",
        "for w in window:\n",
        "    print(f\"=== {w} ===\")\n",
        "    vol_check.plot_acf(residual_ensemble[w], nlags=40, title=f\"ACF - Window {w}\")\n",
        "    vol_check.plot_pacf(residual_ensemble[w], nlags=40, title=f\"PACF - Window {w}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Gr_311R6gEN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gr_311R6gEN",
        "outputId": "92f354cf-31a7-4ad0-8c0a-1f9b2c597145"
      },
      "outputs": [],
      "source": [
        "#window = [504, 756]\n",
        "loss1 = qlike_loss_ensemble[504]\n",
        "loss2 = qlike_loss_ensemble[756]\n",
        "common_idx = loss2.index.intersection( loss1.index)\n",
        "loss2_adj = loss2.loc[common_idx]\n",
        "loss1_adj = loss1.loc[common_idx]\n",
        "\n",
        "DM_test_results = Metric_Evaluation.DM_test(loss1_adj,\n",
        "                                            loss2_adj,\n",
        "                                            model1_name='Window_504',\n",
        "                                            model2_name='Window_756'\n",
        "                                            )\n",
        "\n",
        "print(DM_test_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969e82ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HARX DM test results to report\n",
        "dm_stat_harx, p_val_harx, decision_harx = DM_test_results\n",
        "dm_results_harx = {\n",
        "    \"DM Statistic\": dm_stat_harx,\n",
        "    \"P-value\": p_val_harx,\n",
        "    \"Better Model\": decision_harx['Better model'],\n",
        "    \"Significant?\": decision_harx['Significant'],\n",
        "    \"Alpha\": decision_harx['Alpha'],\n",
        "    \"Observations\": decision_harx['Observations']\n",
        "}\n",
        "report.add_metrics_summary(dm_results_harx, title=\"HARX Model: DM Test Results (Window 504 vs 756)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b7c759",
      "metadata": {},
      "source": [
        "## Comparison of HAR and HARX model results\n",
        "- HARX model shows a tighter range of QLIKE_mean values across different window lengths (252 ‚Üí 1260).\n",
        "- This indicates that HARX performance is more consistent and less sensitive to the choice of rolling window size. HARX‚Äôs use of exogenous variables helps stabilize the model fit across different horizons.\n",
        "- Their absolute QLIKE_mean levels are quite similar (differences ~0.05‚Äì0.15). \n",
        "Comparable overall explanatory power ‚Äî neither dominates strongly across all horizons.\n",
        "- At window = 756, HARX perform slightly better than HAR, suggesting window is large enough to capture long-memory volatility effects, but not so wide that exogenous signals lose relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "epCcGamO6gAN",
      "metadata": {
        "id": "epCcGamO6gAN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "rvR8PNOwJJ-z",
      "metadata": {
        "id": "rvR8PNOwJJ-z"
      },
      "source": [
        "## Test Set Evaluation with HARX Model of window = 756"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7SXsS4gFXLLy",
      "metadata": {
        "id": "7SXsS4gFXLLy"
      },
      "outputs": [],
      "source": [
        "#HAR-504 \n",
        "#HARX-756"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bWCHI0JJNq",
      "metadata": {
        "id": "71bWCHI0JJNq"
      },
      "outputs": [],
      "source": [
        "# HARX: ensemble model with window = 756\n",
        "n_total = len(vol_adj_harx)\n",
        "split_point = int(0.7 * n_total)\n",
        "#x_variables\n",
        "train_x = vol_adj_harx.iloc[:split_point]\n",
        "test_x = vol_adj_harx.iloc[split_point:]\n",
        "\n",
        "exo_harx_train = exo_std_harx_r1.iloc[:split_point]\n",
        "exo_harx_test = exo_std_harx_r1.iloc[split_point:]\n",
        "\n",
        "#y_variables\n",
        "train_y = y_true_log_harx.iloc[:split_point]\n",
        "test_y = y_true_log_harx.iloc[split_point:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KPwR3qrkYvCQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "KPwR3qrkYvCQ",
        "outputId": "9a01db45-1286-458c-f481-30ef3de82f2e"
      },
      "outputs": [],
      "source": [
        "test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VPnYAjZJJIL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VPnYAjZJJIL",
        "outputId": "36090858-7418-4956-eab1-faa364acfb43"
      },
      "outputs": [],
      "source": [
        "w_HARX = 756\n",
        "\n",
        "test_x_aug = pd.concat([train_x.tail(w_HARX),\n",
        "                       test_x]). sort_index()\n",
        "\n",
        "test_exo_aug = pd.concat([exo_harx_train[-w_HARX:],\n",
        "                          exo_harx_test]).sort_index()\n",
        "\n",
        "test_y_aug = pd.concat([train_y[-w_HARX:],\n",
        "                        test_y]).sort_index()\n",
        "print(test_x_aug)\n",
        "print(test_exo_aug)\n",
        "print(test_y_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AvbmPXIagaHm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvbmPXIagaHm",
        "outputId": "6d3bc292-89a2-49b2-8f56-2ee6534ab9f8"
      },
      "outputs": [],
      "source": [
        "test_x_aug.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y135mtSzftyQ",
      "metadata": {
        "id": "y135mtSzftyQ"
      },
      "outputs": [],
      "source": [
        "estimators = ['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']\n",
        "exo_cols = ['UST10Y', 'HYOAS', 'TermSpread_10Y_2Y', 'VIX', 'Breakeven10Y']\n",
        "\n",
        "per_pred = {}\n",
        "per_residual = {}\n",
        "pred_raw_residual = {}\n",
        "qlike_df = {}\n",
        "w_HARX = 756\n",
        "\n",
        "X_all = {}\n",
        "Y_all = {}\n",
        "\n",
        "yhat_est = {est: [] for est in estimators}\n",
        "\n",
        "# first step to get all the features out from test set\n",
        "for est in estimators:\n",
        "  df_in = pd.concat([test_x_aug[[est]], test_exo_aug[exo_cols]], axis =1)\n",
        "  har = HAR_Model(y_log_col=est, exo_col=exo_cols, lags=[1,5,22])\n",
        "\n",
        "  x_est = har.features(df_in) #lags + exo\n",
        "\n",
        "  #to align the index\n",
        "  common_idx = test_y_aug.index.intersection(x_est.index)\n",
        "  x_est = x_est.loc[common_idx]\n",
        "  y_true = test_y_aug.loc[common_idx] # log variance\n",
        "\n",
        "  X_all[est] = x_est\n",
        "  Y_all = y_true\n",
        "  Y_all_var = np.exp(y_true) # variance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ia5FRMgEftp4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia5FRMgEftp4",
        "outputId": "3019ac7b-ef9d-4228-ca6e-0cb8027f8513"
      },
      "outputs": [],
      "source": [
        "print(X_all)\n",
        "print(Y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cg_JyChYqzdF",
      "metadata": {
        "id": "cg_JyChYqzdF"
      },
      "outputs": [],
      "source": [
        "yhat_est = {est: [] for est in estimators}\n",
        "yhat_est_var = {est: [] for est in estimators}\n",
        "yhat_ensemble = []\n",
        "yhat_var_df = []\n",
        "\n",
        "residual_est = {est: [] for est in estimators}\n",
        "residual_ensemble = []\n",
        "\n",
        "q_loss_est = {}\n",
        "\n",
        "qlike_loss_ensemble = []\n",
        "mspe_loss_ensemble = []\n",
        "rmse_loss_ensemble = []\n",
        "\n",
        "weight_est = []\n",
        "weight_path = {est: [] for est in estimators}\n",
        "\n",
        "log_yhat_final = []\n",
        "\n",
        "predict_idx = X_all[estimators[0]].index\n",
        "\n",
        "Y_all_var = np.exp(y_true)\n",
        "\n",
        "#to get rolling window prediction\n",
        "for t in range(w_HARX, len(predict_idx)):\n",
        "  ts = predict_idx[t]\n",
        "  win_start = t-w_HARX\n",
        "  win_end = t\n",
        "\n",
        "  yhat_t = {}\n",
        "  y_pred_t = {}\n",
        "  yhat_var_t = {}\n",
        "  residual_t = {}\n",
        "  raw_residual_t = {}\n",
        "  pred_raw_residual_t = {}\n",
        "  yhat_var = {}\n",
        "  pred_residual_t = {}\n",
        "  pred_raw_residual_t = {}\n",
        "\n",
        "  for est in estimators:\n",
        "    x_window = X_all[est].iloc[win_start:win_end +1]\n",
        "    y_window = Y_all.iloc[win_start:win_end +1]\n",
        "\n",
        "    har = HAR_Model(y_log_col=est, exo_col=exo_cols, lags=[1,5,22])\n",
        "    y_pred, resid_pred, residual_raw = har.fit_predict(x_window,\n",
        "                                                       y_window,\n",
        "                                                       window=w_HARX)\n",
        "\n",
        "    #extraction prediction results at time t\n",
        "    y_pred_t[est] = y_pred.iloc[-1]\n",
        "    residual_t[est] = resid_pred.iloc[-1]\n",
        "    raw_residual_t[est] = residual_raw.iloc[-1]\n",
        "\n",
        "    #store results\n",
        "    yhat_est[est].append((ts, y_pred_t)) #log variance\n",
        "    residual_est[est].append((ts,raw_residual_t)) #residual computed by log variance\n",
        "\n",
        "  y_true_t = Y_all_var.loc[ts] #variance\n",
        "\n",
        "  # nested dict to covert to dataframe\n",
        "  rows = []\n",
        "  for est, tuples in  yhat_est.items():\n",
        "      for ts, inner_dict in tuples:\n",
        "          row = {'Date': ts}\n",
        "          row.update(inner_dict)  # add all estimator predictions\n",
        "          rows.append(row)\n",
        "  yhat_est_df = pd.DataFrame(rows).drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "  yhat_var_row = pd.DataFrame(np.exp(yhat_est_df.loc[ts]))\n",
        "\n",
        "  qlike_loss_df = pd.DataFrame({col: Metric_Evaluation.qlike(y_true_t, yhat_var_row[col])\n",
        "                                for col in yhat_var_row.columns}) # use variance to compute\n",
        "  mspe_loss_df  = pd.DataFrame({col: Metric_Evaluation.mspe(y_true_t, yhat_var_row[col])\n",
        "                                for col in yhat_var_row.columns})  # use variance to compute\n",
        "\n",
        "  qlike_loss_T = qlike_loss_df.T\n",
        "  ensemble_model = EnsembleModel(estimators=list(yhat_var_row.columns))\n",
        "  weight_path[ts] = ensemble_model.compute_weightage(qlike_loss_T.loc[ts]) #get weights for 4 estimators\n",
        "  weight_est.append((ts,weight_path[ts]))\n",
        "  q_loss_est[ts] = qlike_loss_T.loc[ts]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KI9IGXR6jWd-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9IGXR6jWd-",
        "outputId": "8770e16b-631e-4923-871c-e95c6df9ed38"
      },
      "outputs": [],
      "source": [
        "print(weight_est) #weightage\n",
        "print(yhat_est) # log-variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U48SgnFOqzZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U48SgnFOqzZ9",
        "outputId": "2ba43147-3f44-4073-f082-44c3424f697f"
      },
      "outputs": [],
      "source": [
        "#weight\n",
        "rows = []\n",
        "for ts, inner_dict in weight_est:   # directly unpack\n",
        "    row = {'Date': ts}\n",
        "    row.update(inner_dict)          # add weight values for all estimators\n",
        "    rows.append(row)\n",
        "\n",
        "weight_HARX = pd.DataFrame(rows).drop_duplicates(subset=['Date']).set_index('Date')\n",
        "print(weight_HARX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eSsh-dNj1M3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSsh-dNj1M3b",
        "outputId": "3fdb2a76-4666-4341-fb4c-964fc2e470c8"
      },
      "outputs": [],
      "source": [
        "#yhat\n",
        "rows = []\n",
        "# Iterate through the outer dict\n",
        "for est_name, records in yhat_est.items():\n",
        "    for ts, inner_dict in records:\n",
        "        row = {'Date': ts}\n",
        "        row.update(inner_dict)  # add all estimator values\n",
        "        rows.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "yhat_log_harx = pd.DataFrame(rows)\n",
        "\n",
        "# Drop duplicates so each timestamp appears only once\n",
        "yhat_log_harx = yhat_log_harx.drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "yhat_var_harx = np.exp(yhat_log_harx)\n",
        "\n",
        "print(yhat_var_harx)\n",
        "print(yhat_log_harx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eFtPuROjtH80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFtPuROjtH80",
        "outputId": "87c98201-1a60-4f72-bf36-cb4f76a78697"
      },
      "outputs": [],
      "source": [
        "#compute to get the final yhat - variance\n",
        "weights_shifted = weight_HARX.shift(1)\n",
        "weights_shifted.iloc[0] = 1 / len(estimators)\n",
        "\n",
        "yhat_ensemble_HARX = (weights_shifted * yhat_var_harx).sum(axis=1) # variance\n",
        "yhat_ensemble_HARX.name = 'yhat_ensemble_HARX'\n",
        "\n",
        "yhat_ensemble_log_HARX = np.log(yhat_ensemble_HARX)\n",
        "\n",
        "print(yhat_ensemble_HARX)\n",
        "print(yhat_ensemble_log_HARX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y0lKw_XiwUED",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0lKw_XiwUED",
        "outputId": "07cfe0b2-fdd3-4cdb-e6b2-8e5d3968dbf6"
      },
      "outputs": [],
      "source": [
        "# both in log variance to plot later\n",
        "common_idx = y_true_log.index.intersection(yhat_ensemble_log_HARX.index)\n",
        "y_actual = y_true_log.loc[common_idx]\n",
        "yhat_ensemble_HARX_f = yhat_ensemble_log_HARX.loc[common_idx]\n",
        "\n",
        "print(y_actual)\n",
        "print(yhat_ensemble_HARX_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pXR6pX9F3vxj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pXR6pX9F3vxj",
        "outputId": "292ea72c-092a-49d7-f14c-e28cce3ca613"
      },
      "outputs": [],
      "source": [
        "eps = 1e-12\n",
        "\n",
        "y_true =\\\n",
        "(\n",
        "    252\n",
        "    *\n",
        "    (np.log(tlt_data[\"Close\"]\n",
        "           .shift(-1)\n",
        "            /\n",
        "           tlt_data[\"Close\"]))**2\n",
        "\n",
        ")\n",
        "\n",
        "y_actual_var =y_true.loc[common_idx]\n",
        "yhat_ensemble_HARX_var = yhat_ensemble_HARX.loc[common_idx]\n",
        "print(y_actual_var)\n",
        "print(yhat_ensemble_HARX_var)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_var, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HARX_var, label='Predicted Variance (Ensemble HARX)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.legend()\n",
        "plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f912cf31",
      "metadata": {},
      "outputs": [],
      "source": [
        "#lOG VARIANCE SCALE\n",
        "eps = 1e-12\n",
        "\n",
        "y_true_log = np.log(y_true.clip(lower=eps))\n",
        "y_true_log =\\\n",
        "(\n",
        "    y_true_log\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .dropna()\n",
        "    .iloc[1:]\n",
        ")\n",
        "\n",
        "y_actual_log =y_true_log.loc[common_idx]\n",
        "yhat_ensemble_HARX_log = yhat_ensemble_HARX_f.loc[common_idx]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_log, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HARX_log, label='Predicted Variance (Ensemble HARX)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Log Variance\")\n",
        "plt.legend()\n",
        "plt.title(f\"HAR prediction vs true RV for window {w}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dR6lwT60wT7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dR6lwT60wT7c",
        "outputId": "3550f618-c7b6-4227-94a8-12d2d79b8ac3"
      },
      "outputs": [],
      "source": [
        "# model prediction performance\n",
        "qlike_loss_df = pd.DataFrame(Metric_Evaluation.qlike(y_actual_var,yhat_ensemble_HARX_var), columns=['qlike_loss']) # use variance to compute\n",
        "mspe_loss_df  = pd.DataFrame(Metric_Evaluation.mspe(y_actual_var,yhat_ensemble_HARX_var), columns=['mspe_loss'])\n",
        "rmse_loss_df  = pd.DataFrame(Metric_Evaluation.rmse(y_actual_var,yhat_ensemble_HARX_var), columns =['rmse_loss'])\n",
        "\n",
        "rmse_loss_df_adj = rmse_loss_df.dropna()\n",
        "mspe_loss_df_adj = mspe_loss_df.dropna()\n",
        "\n",
        "print(qlike_loss_df)\n",
        "print(mspe_loss_df)\n",
        "print(rmse_loss_df)\n",
        "\n",
        "qlike_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"QLIKE\")\n",
        "plt.legend()\n",
        "plt.title(f\"QLIKE Loss for Ensemble Model HARX\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "mspe_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"MSPE\")\n",
        "plt.legend()\n",
        "plt.title(f\"MSPE Loss for Ensemble Model HARX\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "rmse_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.title(f\"RMSE Loss for Ensemble Model HARX\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KqR1Ro_aA2Wg",
      "metadata": {
        "id": "KqR1Ro_aA2Wg"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_qlike_mean = qlike_loss_df.mean()\n",
        "test_qlike_std = qlike_loss_df.std()\n",
        "test_mspe_mean = mspe_loss_df_adj.mean()\n",
        "test_mspe_std = mspe_loss_df_adj.std()\n",
        "rmse_loss_df_adj_mean = rmse_loss_df_adj.mean()\n",
        "rmse_loss_df_adj_std = rmse_loss_df_adj.std()\n",
        "\n",
        "print(f\"Test QLIKE Mean: {test_qlike_mean.values[0]:.4f}, Std: {test_qlike_std.values[0]:.4f}\")\n",
        "print(f\"Test MSPE  Mean: {test_mspe_mean.values[0]:.4f}, Std: {test_mspe_std.values[0]:.4f}\")\n",
        "print(f\"Test RMSE  Mean: {rmse_loss_df_adj_mean.values[0]:.4f}, Std: {rmse_loss_df_adj_std.values[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abdcac52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HARX test set results to report\n",
        "report.add_section(\"Test Set Evaluation\", level=2)\n",
        "report.add_section(\"HARX Model (Window=756)\", level=3)\n",
        "\n",
        "# Save variance scale plot\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_var, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HARX_var, label='Predicted Variance (Ensemble HARX)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.legend()\n",
        "plt.title(\"HARX Model: Test Set Predictions (Variance Scale)\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"harx_test_variance\", \n",
        "                        caption=\"HARX Test Set: Predictions vs Actual (Variance Scale)\")\n",
        "plt.close()\n",
        "\n",
        "# Save log variance scale plot\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_log, label='Actual Log Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HARX_log, label='Predicted Log Variance (Ensemble HARX)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Log Variance\")\n",
        "plt.legend()\n",
        "plt.title(\"HARX Model: Test Set Predictions (Log Variance Scale)\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"harx_test_log_variance\", \n",
        "                        caption=\"HARX Test Set: Predictions vs Actual (Log Variance Scale)\")\n",
        "plt.close()\n",
        "\n",
        "# Save loss plots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "qlike_loss_df.plot(ax=plt.gca())\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"QLIKE\")\n",
        "plt.title(\"HARX Test Set: QLIKE Loss\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"harx_test_qlike\", caption=\"HARX Test Set: QLIKE Loss Over Time\")\n",
        "plt.close()\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "mspe_loss_df.plot(ax=plt.gca())\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"MSPE\")\n",
        "plt.title(\"HARX Test Set: MSPE Loss\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"harx_test_mspe\", caption=\"HARX Test Set: MSPE Loss Over Time\")\n",
        "plt.close()\n",
        "\n",
        "# Add metrics summary\n",
        "harx_test_metrics = {\n",
        "    \"QLIKE Mean\": test_qlike_mean.values[0],\n",
        "    \"QLIKE Std\": test_qlike_std.values[0],\n",
        "    \"MSPE Mean\": test_mspe_mean.values[0],\n",
        "    \"MSPE Std\": test_mspe_std.values[0],\n",
        "    \"RMSE Mean\": rmse_loss_df_adj_mean.values[0],\n",
        "    \"RMSE Std\": rmse_loss_df_adj_std.values[0]\n",
        "}\n",
        "report.add_metrics_summary(harx_test_metrics, title=\"HARX Test Set Performance Metrics\")\n",
        "\n",
        "print(\"‚úì HARX test set results saved to report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ore5kP5hWqHQ",
      "metadata": {
        "id": "Ore5kP5hWqHQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "yhat_ensemble_HARX_f.to_csv(\"yhat_log_harx.csv\", index=True)\n",
        "yhat_ensemble_HARX_var.to_csv(\"yhat_var_harx.csv\", index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b6f5fd",
      "metadata": {},
      "source": [
        "## Test set to run - HAR model with window = 504"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea13f0a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc612b74",
      "metadata": {},
      "outputs": [],
      "source": [
        "#HAR-504 \n",
        "#HARX-756\n",
        "w_HAR = 504\n",
        "\n",
        "test_x_aug = pd.concat([train_x.tail(w_HAR),\n",
        "                       test_x]). sort_index()\n",
        "\n",
        "test_y_aug = pd.concat([train_y[-w_HAR:],\n",
        "                        test_y]).sort_index()\n",
        "print(test_x_aug)\n",
        "print(test_y_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ea74af",
      "metadata": {},
      "outputs": [],
      "source": [
        "estimators = ['square_est_log', 'parkinson_est_log', 'gk_est_log', 'rs_est_log']\n",
        "\n",
        "per_pred = {}\n",
        "per_residual = {}\n",
        "pred_raw_residual = {}\n",
        "qlike_df = {}\n",
        "X_all = {}\n",
        "Y_all = {}\n",
        "\n",
        "yhat_est = {est: [] for est in estimators}\n",
        "\n",
        "# first step to get all the features out from test set\n",
        "for est in estimators:\n",
        "  har = HAR_Model(y_log_col=est, exo_col=None, lags=[1,5,22])\n",
        "\n",
        "  x_est = har.features(test_x_aug) #lags \n",
        "\n",
        "  #to align the index\n",
        "  common_idx = test_y_aug.index.intersection(x_est.index)\n",
        "  x_est = x_est.loc[common_idx]\n",
        "  y_true = test_y_aug.loc[common_idx] # log variance\n",
        "\n",
        "  X_all[est] = x_est\n",
        "  Y_all = y_true\n",
        "  Y_all_var = np.exp(y_true) # variance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc300f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_est = {est: [] for est in estimators}\n",
        "yhat_est_var = {est: [] for est in estimators}\n",
        "yhat_ensemble = []\n",
        "yhat_var_df = []\n",
        "\n",
        "residual_est = {est: [] for est in estimators}\n",
        "residual_ensemble = []\n",
        "\n",
        "q_loss_est = {}\n",
        "\n",
        "qlike_loss_ensemble = []\n",
        "mspe_loss_ensemble = []\n",
        "rmse_loss_ensemble = []\n",
        "\n",
        "weight_est = []\n",
        "weight_path = {est: [] for est in estimators}\n",
        "\n",
        "log_yhat_final = []\n",
        "\n",
        "predict_idx = X_all[estimators[0]].index\n",
        "\n",
        "Y_all_var = np.exp(y_true)\n",
        "\n",
        "#to get rolling window prediction\n",
        "for t in range(w_HAR, len(predict_idx)):\n",
        "  ts = predict_idx[t]\n",
        "  win_start = t-w_HAR\n",
        "  win_end = t\n",
        "\n",
        "  yhat_t = {}\n",
        "  y_pred_t = {}\n",
        "  yhat_var_t = {}\n",
        "  residual_t = {}\n",
        "  raw_residual_t = {}\n",
        "  pred_raw_residual_t = {}\n",
        "  yhat_var = {}\n",
        "  pred_residual_t = {}\n",
        "  pred_raw_residual_t = {}\n",
        "\n",
        "  for est in estimators:\n",
        "    x_window = X_all[est].iloc[win_start:win_end +1]\n",
        "    y_window = Y_all.iloc[win_start:win_end +1]\n",
        "\n",
        "    y_pred, resid_pred, residual_raw = har.fit_predict(x_window,\n",
        "                                                       y_window,\n",
        "                                                       window=w_HAR)\n",
        "\n",
        "    #extraction prediction results at time t\n",
        "    y_pred_t[est] = y_pred.iloc[-1]\n",
        "    residual_t[est] = resid_pred.iloc[-1]\n",
        "    raw_residual_t[est] = residual_raw.iloc[-1]\n",
        "\n",
        "    #store results\n",
        "    yhat_est[est].append((ts, y_pred_t)) #log variance\n",
        "    residual_est[est].append((ts,raw_residual_t)) #residual computed by log variance\n",
        "\n",
        "  y_true_t = Y_all_var.loc[ts] #variance\n",
        "\n",
        "  # nested dict to covert to dataframe\n",
        "  rows = []\n",
        "  for est, tuples in  yhat_est.items():\n",
        "      for ts, inner_dict in tuples:\n",
        "          row = {'Date': ts}\n",
        "          row.update(inner_dict)  # add all estimator predictions\n",
        "          rows.append(row)\n",
        "  yhat_est_df = pd.DataFrame(rows).drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "  yhat_var_row = pd.DataFrame(np.exp(yhat_est_df.loc[ts]))\n",
        "\n",
        "  qlike_loss_df = pd.DataFrame({col: Metric_Evaluation.qlike(y_true_t, yhat_var_row[col])\n",
        "                                for col in yhat_var_row.columns}) # use variance to compute\n",
        "  mspe_loss_df  = pd.DataFrame({col: Metric_Evaluation.mspe(y_true_t, yhat_var_row[col])\n",
        "                                for col in yhat_var_row.columns})  # use variance to compute\n",
        "\n",
        "  qlike_loss_T = qlike_loss_df.T\n",
        "  ensemble_model = EnsembleModel(estimators=list(yhat_var_row.columns))\n",
        "  weight_path[ts] = ensemble_model.compute_weightage(qlike_loss_T.loc[ts]) #get weights for 4 estimators\n",
        "  weight_est.append((ts,weight_path[ts]))\n",
        "  q_loss_est[ts] = qlike_loss_T.loc[ts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322db424",
      "metadata": {},
      "outputs": [],
      "source": [
        "#weight\n",
        "rows = []\n",
        "for ts, inner_dict in weight_est:   # directly unpack\n",
        "    row = {'Date': ts}\n",
        "    row.update(inner_dict)          # add weight values for all estimators\n",
        "    rows.append(row)\n",
        "\n",
        "weight_HAR = pd.DataFrame(rows).drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "\n",
        "#yhat\n",
        "rows = []\n",
        "# Iterate through the outer dict\n",
        "for est_name, records in yhat_est.items():\n",
        "    for ts, inner_dict in records:\n",
        "        row = {'Date': ts}\n",
        "        row.update(inner_dict)  # add all estimator values\n",
        "        rows.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "yhat_log_har = pd.DataFrame(rows)\n",
        "\n",
        "# Drop duplicates so each timestamp appears only once\n",
        "yhat_log_har = yhat_log_har.drop_duplicates(subset=['Date']).set_index('Date')\n",
        "\n",
        "yhat_var_har = np.exp(yhat_log_har)\n",
        "\n",
        "#compute to get the final yhat - variance\n",
        "weights_shifted = weight_HAR.shift(1)\n",
        "weights_shifted.iloc[0] = 1 / len(estimators)\n",
        "\n",
        "yhat_ensemble_HAR = (weights_shifted * yhat_var_har).sum(axis=1) # variance\n",
        "yhat_ensemble_HAR.name = 'yhat_ensemble_HAR'\n",
        "\n",
        "yhat_ensemble_log_HAR = np.log(yhat_ensemble_HAR)\n",
        "\n",
        "print(weight_HAR)\n",
        "print(yhat_var_har)\n",
        "print(yhat_log_har)\n",
        "print(yhat_ensemble_HAR)\n",
        "print(yhat_ensemble_log_HAR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b3cf395",
      "metadata": {},
      "outputs": [],
      "source": [
        "# both in log variance to plot later\n",
        "common_idx = y_true_log.index.intersection(yhat_ensemble_log_HAR.index)\n",
        "y_actual = y_true_log.loc[common_idx]\n",
        "yhat_ensemble_HAR_f = yhat_ensemble_log_HAR.loc[common_idx]\n",
        "\n",
        "eps = 1e-12\n",
        "\n",
        "y_true =\\\n",
        "(\n",
        "    252\n",
        "    *\n",
        "    (np.log(tlt_data[\"Close\"]\n",
        "           .shift(-1)\n",
        "            /\n",
        "           tlt_data[\"Close\"]))**2\n",
        "\n",
        ")\n",
        "\n",
        "y_actual_var =y_true.loc[common_idx]\n",
        "yhat_ensemble_HAR_var = yhat_ensemble_HAR.loc[common_idx]\n",
        "\n",
        "print(y_actual)\n",
        "print(yhat_ensemble_HAR_f)\n",
        "print(y_actual_var)\n",
        "print(yhat_ensemble_HAR_var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60fab0d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VARIANCE SCALE PLOT\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_var, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HAR_var, label='Predicted Variance (Ensemble HAR)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.legend()\n",
        "plt.title(f\"HAR prediction vs true RV for window {w_HAR}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e40e7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOG VARIANCE SCALE PLOT\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_log_HAR, label='Predicted Variance (Ensemble HAR)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Log Variance\")\n",
        "plt.legend()\n",
        "plt.title(f\"HAR prediction vs true RV for window {w_HAR}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695679a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# model prediction performance\n",
        "qlike_loss_df = pd.DataFrame(Metric_Evaluation.qlike(y_actual_var,yhat_ensemble_HAR_var), columns=['qlike_loss']) # use variance to compute\n",
        "mspe_loss_df  = pd.DataFrame(Metric_Evaluation.mspe(y_actual_var,yhat_ensemble_HAR_var), columns=['mspe_loss'])\n",
        "rmse_loss_df  = pd.DataFrame(Metric_Evaluation.rmse(y_actual_var,yhat_ensemble_HAR_var), columns =['rmse_loss'])\n",
        "\n",
        "rmse_loss_df_adj = rmse_loss_df.dropna()\n",
        "mspe_loss_df_adj = mspe_loss_df.dropna()\n",
        "\n",
        "print(qlike_loss_df)\n",
        "print(mspe_loss_df)\n",
        "print(rmse_loss_df)\n",
        "\n",
        "qlike_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"QLIKE\")\n",
        "plt.legend()\n",
        "plt.title(f\"QLIKE Loss for Ensemble Model HAR\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "mspe_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"MSPE\")\n",
        "plt.legend()\n",
        "plt.title(f\"MSPE Loss for Ensemble Model HAR\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "rmse_loss_df.plot()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.title(f\"RMSE Loss for Ensemble Model HAR\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4eb504d",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_qlike_mean = qlike_loss_df.mean()\n",
        "test_qlike_std = qlike_loss_df.std()\n",
        "test_mspe_mean = mspe_loss_df_adj.mean()\n",
        "test_mspe_std = mspe_loss_df_adj.std()\n",
        "rmse_loss_df_adj_mean = rmse_loss_df_adj.mean()\n",
        "rmse_loss_df_adj_std = rmse_loss_df_adj.std()\n",
        "\n",
        "print(f\"Test QLIKE Mean: {test_qlike_mean.values[0]:.4f}, Std: {test_qlike_std.values[0]:.4f}\")\n",
        "print(f\"Test MSPE  Mean: {test_mspe_mean.values[0]:.4f}, Std: {test_mspe_std.values[0]:.4f}\")\n",
        "print(f\"Test RMSE  Mean: {rmse_loss_df_adj_mean.values[0]:.4f}, Std: {rmse_loss_df_adj_std.values[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9600cc1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add HAR test set results to report\n",
        "report.add_section(\"HAR Model (Window=504)\", level=3)\n",
        "\n",
        "# Save variance scale plot\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual_var, label='Actual Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_HAR_var, label='Predicted Variance (Ensemble HAR)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.legend()\n",
        "plt.title(\"HAR Model: Test Set Predictions (Variance Scale)\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"har_test_variance\", \n",
        "                        caption=\"HAR Test Set: Predictions vs Actual (Variance Scale)\")\n",
        "plt.close()\n",
        "\n",
        "# Save log variance scale plot\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_actual, label='Actual Log Variance', color='blue')\n",
        "plt.plot(yhat_ensemble_log_HAR, label='Predicted Log Variance (Ensemble HAR)', color='orange')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Log Variance\")\n",
        "plt.legend()\n",
        "plt.title(\"HAR Model: Test Set Predictions (Log Variance Scale)\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"har_test_log_variance\", \n",
        "                        caption=\"HAR Test Set: Predictions vs Actual (Log Variance Scale)\")\n",
        "plt.close()\n",
        "\n",
        "# Save loss plots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "qlike_loss_df.plot(ax=plt.gca())\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"QLIKE\")\n",
        "plt.title(\"HAR Test Set: QLIKE Loss\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"har_test_qlike\", caption=\"HAR Test Set: QLIKE Loss Over Time\")\n",
        "plt.close()\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "mspe_loss_df.plot(ax=plt.gca())\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"MSPE\")\n",
        "plt.title(\"HAR Test Set: MSPE Loss\")\n",
        "plt.tight_layout()\n",
        "report.save_and_add_plot(fig, \"har_test_mspe\", caption=\"HAR Test Set: MSPE Loss Over Time\")\n",
        "plt.close()\n",
        "\n",
        "# Add metrics summary\n",
        "har_test_metrics = {\n",
        "    \"QLIKE Mean\": test_qlike_mean.values[0],\n",
        "    \"QLIKE Std\": test_qlike_std.values[0],\n",
        "    \"MSPE Mean\": test_mspe_mean.values[0],\n",
        "    \"MSPE Std\": test_mspe_std.values[0],\n",
        "    \"RMSE Mean\": rmse_loss_df_adj_mean.values[0],\n",
        "    \"RMSE Std\": rmse_loss_df_adj_std.values[0]\n",
        "}\n",
        "report.add_metrics_summary(har_test_metrics, title=\"HAR Test Set Performance Metrics\")\n",
        "\n",
        "print(\"‚úì HAR test set results saved to report\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404d23bf",
      "metadata": {},
      "source": [
        "## Preliminary Evaluation\n",
        "- HARX only provide slight improvement as compared to HAR model\n",
        "- HARX offers a more stable, better-calibrated prediction with near-equivalent RMSE and smoother QLIKE/MSPE behavior, especially evident around the 756-day horizon.\n",
        "- The marginal contribution of exogenous variables (HARX) is expected to be small unless those variables provide substantial new information orthogonal to past volatility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910613dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_ensemble_log_HAR.to_csv(\"yhat_log_har.csv\", index=True)\n",
        "yhat_ensemble_HAR_var.to_csv(\"yhat_var_har.csv\", index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d879bae",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d9a40b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalize the report with conclusions\n",
        "report.add_section(\"Model Comparison\", level=2)\n",
        "report.add_text(\"\"\"\n",
        "### Key Findings from HAR vs HARX Comparison\n",
        "\n",
        "**1. Performance Consistency**\n",
        "- HARX model shows tighter range of QLIKE_mean values across different window lengths (252 ‚Üí 1260)\n",
        "- HARX performance is more consistent and less sensitive to window size choice\n",
        "- Exogenous variables help stabilize model fit across different horizons\n",
        "\n",
        "**2. Overall Predictive Power**\n",
        "- Absolute QLIKE_mean levels are similar between HAR and HARX (differences ~0.05‚Äì0.15)\n",
        "- Neither model dominates strongly across all horizons\n",
        "- Comparable explanatory power for both approaches\n",
        "\n",
        "**3. Optimal Window Selection**\n",
        "- At window = 756, HARX performs slightly better than HAR\n",
        "- Window is large enough to capture long-memory volatility effects\n",
        "- Window not so wide that exogenous signals lose relevance\n",
        "\n",
        "**4. Marginal Contribution of Exogenous Variables**\n",
        "- HARX offers marginal improvement over HAR\n",
        "- More stable and better-calibrated predictions\n",
        "- Smoother QLIKE/MSPE behavior, especially around 756-day horizon\n",
        "\"\"\")\n",
        "\n",
        "report.add_section(\"Conclusions\", level=2)\n",
        "report.add_text(\"\"\"\n",
        "### Summary of Results\n",
        "\n",
        "**Ensemble Model Performance**\n",
        "- Ensemble models generally outperform individual estimators in QLIKE, MSPE, and volatility/stability\n",
        "- Inverse QLIKE weighting provides effective combination of multiple estimators\n",
        "- RS estimator consistently performs worst and was excluded from ensemble\n",
        "\n",
        "**Residual Diagnostics**\n",
        "- All windows passed Ljung-Box test for ensemble models\n",
        "- Windows 756/1008 show highest p-values (least autocorrelation)\n",
        "- PACF and ACF plots show no significant autocorrelation beyond lag 0\n",
        "- Residuals behave like white noise, indicating good model fit\n",
        "\n",
        "**Test Set Performance**\n",
        "- HARX (window=756) provides slight improvement over HAR (window=504)\n",
        "- HARX offers more stable predictions with near-equivalent RMSE\n",
        "- Both models demonstrate strong out-of-sample forecasting ability\n",
        "\n",
        "**Statistical Validation**\n",
        "- Diebold-Mariano tests show no statistically significant differences between windows 504 and 756\n",
        "- Model selection can be based on secondary metrics or practical considerations\n",
        "- Both HAR and HARX are valid approaches for volatility forecasting\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **For Production Use**: HARX model with window=756 recommended for most stable performance\n",
        "2. **For Simplicity**: HAR model with window=504 provides comparable results with fewer inputs\n",
        "3. **For Research**: Both models provide solid baseline for further enhancement\n",
        "4. **For Ensemble**: Inverse QLIKE weighting continues to be effective combination strategy\n",
        "\"\"\")\n",
        "\n",
        "report.add_section(\"Appendix\", level=2)\n",
        "report.add_text(\"\"\"\n",
        "### Volatility Estimators Used\n",
        "\n",
        "1. **Squared Return (RV)**: Classic realized volatility based on squared returns\n",
        "2. **Parkinson Estimator**: Range-based estimator using high-low prices\n",
        "3. **Garman-Klass (GK) Estimator**: Drift-adjusted range-based estimator\n",
        "4. **Rogers-Satchell (RS) Estimator**: Allows for drift in price process\n",
        "\n",
        "### HAR Model Specification\n",
        "\n",
        "The Heterogeneous Autoregressive (HAR) model captures volatility at multiple time scales:\n",
        "- **Daily component (lag 1)**: Short-term volatility effects\n",
        "- **Weekly component (lag 5)**: Medium-term volatility patterns\n",
        "- **Monthly component (lag 22)**: Long-term volatility trends\n",
        "\n",
        "### Exogenous Variables (HARX)\n",
        "\n",
        "- **UST10Y**: 10-Year US Treasury Yield (interest rate environment)\n",
        "- **HYOAS**: High Yield Option-Adjusted Spread (credit risk premium)\n",
        "- **TermSpread_10Y_2Y**: Term Spread (yield curve shape)\n",
        "- **VIX**: CBOE Volatility Index (market fear gauge)\n",
        "- **Breakeven10Y**: 10-Year Breakeven Inflation Rate (inflation expectations)\n",
        "\n",
        "### Metrics\n",
        "\n",
        "- **QLIKE**: Quasi-likelihood loss function, measures forecast calibration\n",
        "- **MSPE**: Mean squared prediction error, measures raw forecast error magnitude\n",
        "- **RMSE**: Root mean squared error, interpretable scale for forecast errors\n",
        "\"\"\")\n",
        "\n",
        "# Finalize the report\n",
        "report.finalize_report()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REPORT GENERATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nYour comprehensive volatility forecasting report is ready.\")\n",
        "print(f\"This report includes:\")\n",
        "print(f\"  ‚úì Executive summary and methodology\")\n",
        "print(f\"  ‚úì All diagnostic tests and tables\")\n",
        "print(f\"  ‚úì {len(list(report.images_dir.glob('*.png')))} high-quality plots\")\n",
        "print(f\"  ‚úì Performance metrics for all models\")\n",
        "print(f\"  ‚úì Statistical tests and comparisons\")\n",
        "print(f\"  ‚úì Test set evaluation results\")\n",
        "print(f\"  ‚úì Conclusions and recommendations\")\n",
        "print(f\"\\nUse this report as a blueprint for your final presentation!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dad381f",
      "metadata": {},
      "source": [
        "## üìä How to Use the Generated Report\n",
        "\n",
        "### Quick Start Guide\n",
        "\n",
        "1. **Run all cells in this notebook** from top to bottom\n",
        "2. **Find your report** in the `report_output/` directory\n",
        "3. **Open the markdown file** in any markdown viewer or VS Code\n",
        "4. **All images** are saved in `report_output/images/`\n",
        "\n",
        "### What Gets Generated\n",
        "\n",
        "‚úÖ **Comprehensive Markdown Report** with:\n",
        "- Executive summary and methodology\n",
        "- All diagnostic tests and statistical tables  \n",
        "- 60+ high-quality plots and visualizations\n",
        "- Performance metrics for all models\n",
        "- Statistical comparisons (Diebold-Mariano tests)\n",
        "- Test set evaluation results\n",
        "- Conclusions and recommendations\n",
        "\n",
        "‚úÖ **Professional Formatting** ready for:\n",
        "- Conversion to PDF (using Pandoc or similar)\n",
        "- Direct presentation use\n",
        "- Thesis/dissertation inclusion\n",
        "- Research paper drafts\n",
        "\n",
        "### Converting to Other Formats\n",
        "\n",
        "**To PDF:**\n",
        "```bash\n",
        "pandoc report_output/volatility_forecast_report_*.md -o final_report.pdf --pdf-engine=xelatex\n",
        "```\n",
        "\n",
        "**To HTML:**\n",
        "```bash\n",
        "pandoc report_output/volatility_forecast_report_*.md -o final_report.html -s --self-contained\n",
        "```\n",
        "\n",
        "**To Word:**\n",
        "```bash\n",
        "pandoc report_output/volatility_forecast_report_*.md -o final_report.docx\n",
        "```\n",
        "\n",
        "### Customization\n",
        "\n",
        "You can modify the `VolatilityReportGenerator` class to:\n",
        "- Change the report structure\n",
        "- Add more sections\n",
        "- Customize plot styles\n",
        "- Modify table formatting\n",
        "- Add additional metrics\n",
        "\n",
        "### Tips for Presentation\n",
        "\n",
        "1. The report is structured as a complete research document\n",
        "2. Each section can be used as a slide in your presentation\n",
        "3. All plots are high-resolution (150 DPI) and publication-ready\n",
        "4. Tables are formatted in markdown for easy conversion\n",
        "5. Use the Table of Contents to navigate the report\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This automated report generation ensures reproducibility and consistency across all your analyses!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
